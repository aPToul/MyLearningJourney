{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bleach\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.gyford.com/phil/writing/2015/03/25/wikipedia-parsing.php\n",
    "\n",
    "class WikipediaFetcher(object):\n",
    "\n",
    "    def fetch(self, page_name):\n",
    "        \"\"\"\n",
    "        Passed a Wikipedia page's URL fragment, like\n",
    "        'Edward_Montagu,_1st_Earl_of_Sandwich', this will fetch the page's\n",
    "        main contents, tidy the HTML, strip out any elements we don't want\n",
    "        and return the final HTML string.\n",
    "\n",
    "        Returns a dict with two elements:\n",
    "            'success' is either True or, if we couldn't fetch the page, False.\n",
    "            'content' is the HTML if success==True, or else an error message.\n",
    "        \"\"\"\n",
    "        result = self._get_html(page_name)\n",
    "        \n",
    "        if result['success']:\n",
    "            result['content'] = self._tidy_html(result['content'])\n",
    "            \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def _get_html(self, page_name):\n",
    "        \"\"\"\n",
    "        Passed the name of a Wikipedia page (eg, 'Samuel_Pepys'), it fetches\n",
    "        the HTML content (not the entire HTML page) and returns it.\n",
    "\n",
    "        Returns a dict with two elements:\n",
    "            'success' is either True or, if we couldn't fetch the page, False.\n",
    "            'content' is the HTML if success==True, or else an error message.\n",
    "        \"\"\"\n",
    "        error_message = ''\n",
    "\n",
    "        url = 'https://en.wikipedia.org/wiki/%s' % page_name\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params={'action':'render'}, timeout=5)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            error_message = \"Can't connect to domain.\"\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            error_message = \"Connection timed out.\"\n",
    "        except requests.exceptions.TooManyRedirects as e:\n",
    "            error_message = \"Too many redirects.\"\n",
    "\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # 4xx or 5xx errors:\n",
    "            error_message = \"HTTP Error: %s\" % response.status_code\n",
    "        except NameError:\n",
    "            if error_message == '':\n",
    "                error_message = \"Something unusual went wrong.\"\n",
    "\n",
    "        if error_message:\n",
    "            return {'success': False, 'content': error_message} \n",
    "        else:\n",
    "            return {'success': True, 'content': response.text}\n",
    "\n",
    "    def _tidy_html(self, html):\n",
    "        \"\"\"\n",
    "        Passed the raw Wikipedia HTML, this returns valid HTML, with all\n",
    "        disallowed elements stripped out.\n",
    "        \"\"\"\n",
    "        #html = self._bleach_html(html)\n",
    "        #html = self._strip_html(html)\n",
    "        return html\n",
    "\n",
    "    def _bleach_html(self, html):\n",
    "        \"\"\"\n",
    "        Ensures we have valid HTML; no unclosed or mis-nested tags.\n",
    "        Removes any tags and attributes we don't want to let through.\n",
    "        Doesn't remove the contents of any disallowed tags.\n",
    "\n",
    "        Pass it an HTML string, it'll return the bleached HTML string.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pretty much most elements, but no forms or audio/video.\n",
    "        allowed_tags = [\n",
    "            'a', 'abbr', 'acronym', 'address', 'area', 'article',\n",
    "            'b', 'blockquote', 'br',\n",
    "            'caption', 'cite', 'code', 'col', 'colgroup',\n",
    "            'dd', 'del', 'dfn', 'div', 'dl', 'dt',\n",
    "            'em',\n",
    "            'figcaption', 'figure', 'footer',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'header', 'hgroup', 'hr',\n",
    "            'i', 'img', 'ins',\n",
    "            'kbd',\n",
    "            'li',\n",
    "            'map',\n",
    "            'nav',\n",
    "            'ol',\n",
    "            'p', 'pre',\n",
    "            'q',\n",
    "            's', 'samp', 'section', 'small', 'span', 'strong', 'sub', 'sup',\n",
    "            'table', 'tbody', 'td', 'tfoot', 'th', 'thead', 'time', 'tr',\n",
    "            'ul',\n",
    "            'var',\n",
    "        ]\n",
    "\n",
    "        # These attributes will be removed from any of the allowed tags.\n",
    "        allowed_attributes = {\n",
    "            '*':        ['class', 'id'],\n",
    "            'a':        ['href', 'title'],\n",
    "            'abbr':     ['title'],\n",
    "            'acronym':  ['title'],\n",
    "            'img':      ['alt', 'src', 'srcset'],\n",
    "            # Ugh. Don't know why this page doesn't use .tright like others\n",
    "            # http://127.0.0.1:8000/encyclopedia/5040/\n",
    "            'table':    ['align'],\n",
    "            'td':       ['colspan', 'rowspan'],\n",
    "            'th':       ['colspan', 'rowspan', 'scope'],\n",
    "        }\n",
    "\n",
    "        return bleach.clean(html, tags=allowed_tags,\n",
    "                                    attributes=allowed_attributes, strip=True)\n",
    "\n",
    "    def _strip_html(self, html):\n",
    "        \"\"\"\n",
    "        Takes out any tags, and their contents, that we don't want at all.\n",
    "        And adds custom classes to existing tags (so we can apply CSS styles\n",
    "        without having to multiply our CSS).\n",
    "\n",
    "        Pass it an HTML string, it returns the stripped HTML string.\n",
    "        \"\"\"\n",
    "\n",
    "        # CSS selectors. Strip these and their contents.\n",
    "        selectors = [\n",
    "            'div.hatnote',\n",
    "            'div.navbar.mini', # Will also match div.mini.navbar\n",
    "            # Bottom of https://en.wikipedia.org/wiki/Charles_II_of_England :\n",
    "            'div.topicon',\n",
    "            'a.mw-headline-anchor',\n",
    "        ]\n",
    "\n",
    "        # Strip any element that has one of these classes.\n",
    "        classes = [\n",
    "            # \"This article may be expanded with text translated from...\"\n",
    "            # https://en.wikipedia.org/wiki/Afonso_VI_of_Portugal\n",
    "            'ambox-notice',\n",
    "            'magnify',\n",
    "            # eg audio on https://en.wikipedia.org/wiki/Bagpipes\n",
    "            'mediaContainer',\n",
    "            'navbox',\n",
    "            'noprint',\n",
    "        ]\n",
    "\n",
    "        # Any element has a class matching a key, it will have the classes\n",
    "        # in the value added.\n",
    "        add_classes = {\n",
    "            # Give these tables standard Bootstrap styles.\n",
    "            'infobox':   ['table', 'table-bordered'],\n",
    "            'ambox':     ['table', 'table-bordered'],\n",
    "            'wikitable': ['table', 'table-bordered'],\n",
    "        } \n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        for selector in selectors:\n",
    "            [tag.decompose() for tag in soup.select(selector)]\n",
    "\n",
    "        for clss in classes:\n",
    "            [tag.decompose() for tag in soup.find_all(attrs={'class':clss})]\n",
    "\n",
    "        for clss, new_classes in add_classes.iteritems():\n",
    "            for tag in soup.find_all(attrs={'class':clss}):\n",
    "                tag['class'] = tag.get('class', []) + new_classes\n",
    "\n",
    "        # Depending on the HTML parser BeautifulSoup used, soup may have\n",
    "        # surrounding <html><body></body></html> or just <body></body> tags.\n",
    "        if soup.body:\n",
    "            soup = soup.body\n",
    "        elif soup.html:\n",
    "            soup = soup.html.body\n",
    "\n",
    "        # Put the content back into a string.\n",
    "        html = ''.join(str(tag) for tag in soup.contents)\n",
    "\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_list = [\"Beta distribution\", \"Bayesian network\", \"Concentration parameter\", \"Conjugate prior\", \"Directed acyclic graph\", \"Dirichlet distribution\", \"Elastic energy\", \"Elastic map\", \"Elastic net regularization\", \"Expectation–maximization algorithm\", \"Exponential family\", \"Gibbs sampling\", \"Graphical model\", \"Joint probability distribution\", \"Mixture model\", \"Pólya urn model\", \"Posterior probability\", \"Statistical inference\", \"Sufficient statistic\", \"Tikhonov regularization\", \"Variational Bayesian methods\", \"Accelerated failure time model\", \"Akaike information criterion\", \"Arnoldi iteration\", \"Artificial intelligence\", \"Association rule learning\", \"Automatic summarization\", \"Auxiliary verb\", \"Bag of words model\", \"Bateman–Horn conjecture\", \"Bell state\", \"Bernstein–von Mises theorem\", \"Beta-binomial distribution\", \"Beta prime distribution\", \"BHT algorithm\", \"Binary Independence Model\", \"Binomial distribution\", \"Birthday problem\", \"Bit array\", \"Bitwise operation\", \"Bloch sphere\", \"Boosting_(machine_learning)\", \"Bootstrapping\", \"Brachistochrone curve\", \"Bregman divergence\", \"Brown clustering\", \"Bunyakovsky conjecture\", \"Calculus of variations\", \"Canonical correlation\", \"CAP theorem\", \"Catenary\", \"Censored regression model\", \"Censoring_(statistics)\", \"Chi squared distribution\", \"Chi squared test\", \"Chinese restaurant process\", \"Cluster analysis\", \"Cohen's kappa\", \"Collinearity\", \"Collision problem\", \"Color wheel graphs of complex functions\", \"Competitive learning\", \"Complex-valued function\", \"Conditional probability\", \"Conditional quantum entropy\", \"Condorcet method\", \"Confusion matrix\", \"Coprime integers\", \"Cosine similarity\", \"Cross-validation_(statistics)\", \"Cross entropy\", \"Cross entropy method\", \"Cumulative distribution function\", \"Data compression\", \"Data science\", \"Decision engineering\", \"Decision tree learning\", \"Decision tree model\", \"Density estimation\", \"Diagonalizable matrix\", \"Digamma function\", \"Dirac delta function\", \"Dirichlet process\", \"Dirichlet-multinomial_distribution\", \"Dirichlet's theorem on arithmetic progressions\", \"Document clustering\", \"Domain coloring\", \"Doob's martingale convergence theorems\", \"Eigendecomposition of a matrix\", \"Elementary matrix\", \"Empirical distribution function\", \"Ensemble learning\", \"Entropy\", \"Erlang distribution\", \"Exponential distribution\", \"Exponential function\", \"F1 score\", \"Facility location problem\", \"Feedforward neural network\", \"Fermat's factorization method\", \"Fibonacci prime\", \"Fieller's theorem\", \"Floor and ceiling functions\", \"Formula for primes\", \"Fuzzy clustering\", \"Gamma distribution\", \"Gamma function\", \"Gaussian integer\", \"Generalized minimal residual method\", \"Generalized second price auction\", \"Generalized Tobit\", \"Genetic drift\", \"Geodesic\", \"Gini coefficient\", \"Goal programming\", \"Goldbach's conjecture\", \"Gradient boosting\", \"Grammatical modifier\", \"Grover's algorithm\", \"Hash table\", \"Heuristic\", \"Hinge loss\", \"Holevo's theorem\", \"Huber loss\", \"Hypercube\", \"Hyperrectangle\", \"Image processing\", \"Infimum and supremum\", \"Information gain in decision trees\", \"Information gain ratio\", \"Information retrieval\", \"Information theory\", \"Inner product space\", \"Integrated circuit\", \"Integration by substitution\", \"Inverse function theorem\", \"Inverse transform sampling\", \"Inverted index\", \"Irreducible polynomial\", \"Isoperimetric inequality\", \"Jackson network\", \"Jacobian matrix and determinant\", \"Karmarkar's algorithm\", \"Kendall's notation\", \"Kernel method\", \"Kronecker delta\", \"Kullback Leibler divergence\", \"Lagrange multiplier\", \"Lagrangian relaxation\", \"Lasso (statistics)\", \"Latent Dirichlet allocation\", \"Latent semantic analysis\", \"Latent semantic indexing\", \"Least absolute deviations\", \"Least squares\", \"Lemmatisation\", \"Lexeme\", \"Lexical hypothesis\", \"Lexicographical order\", \"Likelihood function\", \"Lindley equation\", \"Linear congruential generator\", \"Log normal distribution\", \"Logarithmic integral function\", \"Logistic regression\", \"Loss function\", \"Loss functions for classification\", \"Machine learning\", \"Markov chain Monte Carlo\", \"Mathematical notation\", \"Matthews correlation coefficient\", \"Mean sojourn time\", \"Meta learning\", \"Metaheuristic\", \"Method of moments (statistics)\", \"Minimal surface\", \"Minimax\", \"Minimax Condorcet\", \"Modal verb\", \"Modular arithmetic\", \"Moran process\", \"morpheme\", \"Morphological derivation\", \"Multidimensional scaling\", \"Multilevel model\", \"Mutilated chessboard problem\", \"Naive Bayes classifier\", \"Negative binomial distribution\", \"Neutral vector\", \"No teleportation theorem\", \"Nonlinear dimensionality reduction\", \"Normal distribution\", \"Notation in probability and statistics\", \"Null hypothesis\", \"Occam's razor\", \"Okapi BM25\", \"Ontology\", \"Open set\", \"Optimal control\", \"Orthogonal transformation\", \"Overfitting\", \"PageRank\", \"Pairwise comparison\", \"Pareto distribution\", \"Part of speech\", \"pattern\", \"Piecewise linear function\", \"Pink noise\", \"Place and route\", \"Plateau's problem\", \"Pochhammer symbol\", \"Poisson distribution\", \"Poisson limit theorem\", \"Poisson process\", \"Positive definite kernel\", \"Power iteration\", \"Precision and recall\", \"Prime element\", \"Prime number theorem\", \"Principal component analysis\", \"Prior probability\", \"Probabilistic latent semantic analysis\", \"Probabilistic relevance model\", \"Probability Generating Function\", \"Probability integral transform\", \"Proportional hazards model\", \"Psychometrics\", \"QR algorithm\", \"Quantum annealing\", \"Quantum field theory\", \"Quantum gate\", \"Quantum information\", \"Quantum information science\", \"Quantum Monte Carlo\", \"Qubit\", \"Query optimization\", \"Queueing theory\", \"Quotient ring\", \"Radial basis function\", \"Random forest\", \"Random variate\", \"Ranking\", \"Real number\", \"Receiver operating characteristic\", \"Relevance\", \"Residual sum of squares\", \"Revised simplex method\", \"Riemann surface\", \"Riemann zeta function\", \"Robust statistics\", \"Root of unity\", \"Run length encoding\", \"Scheffé's method\", \"Schönhage Strassen algorithm\", \"Search engine indexing\", \"Segmented regression\", \"Self information\", \"Self organizing map\", \"Semantic search\", \"Semantics\", \"Semiparametric model\", \"Sentiment analysis\", \"Sigmoid function\", \"Simpson's paradox\", \"Simulated annealing\", \"Singular value decomposition\", \"Skewness\", \"Softmax function\", \"Solved game\", \"Spectral theorem\", \"Standard deviation\", \"Standard error\", \"Stationary point\", \"Statistical ensemble\", \"Statistical power\", \"Stochastic gradient descent\", \"Stop words\", \"Stretched exponential function\", \"Suffix tree\", \"Support vector machine\", \"Survival analysis\", \"Swanson's law\", \"Symmetric derivative\", \"Synonym ring\", \"Text corpus\", \"Tf–idf\", \"Tobit model\", \"Topic model\", \"Torus\", \"Travelling salesman problem\", \"Triangular distribution\", \"Trigamma function\", \"Truncated regression model\", \"Truncation\", \"Twin prime\", \"Type I and type II errors\", \"Ulam spiral\", \"Variance\", \"Vector quantization\", \"Von Neumann entropy\", \"Web crawler\", \"Web search engine\", \"Web search query\", \"Weibull distribution\", \"Zipf's law\", \"Dirichlet-multinomial distribution\", \"F-test of equality of variances\", \"K-means_clustering\", \"n-gram\", \"Rayleigh–Ritz method\", \"Student's t-test\"]\n",
    "\n",
    "document_set = {}\n",
    "\n",
    "for article in article_list:\n",
    "    myScraper = WikipediaFetcher()\n",
    "    scraped_article = myScraper.fetch(page_name=article)\n",
    "    if scraped_article['success'] == True:\n",
    "        soup = BeautifulSoup(scraped_article['content'], 'html.parser')\n",
    "        \n",
    "        document_text = \"\"        \n",
    "        get_tags = soup.find_all(['p'])        \n",
    "        for tag in get_tags:    \n",
    "            clean_text = (''.join(tag.findAll(text=True))).strip()            \n",
    "            document_text = document_text + \" \" + clean_text    \n",
    "        \n",
    "        if len(document_text) > 250:\n",
    "            document_set[article]  = document_text\n",
    "        else:\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print article + \" was too short: \"\n",
    "            print \"\"\n",
    "            print document_text\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print \"\"\n",
    "    else:\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print article + \" failed to scrape\"\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_rows = soup.find_all(['li', 'p'])\n",
    "\n",
    "# section_text = {}\n",
    "\n",
    "# current_section = \"Wiki_Introduction\"\n",
    "# section_text[current_section] = \"\"\n",
    "\n",
    "# for row in get_rows:    \n",
    "#     potential_section = row.find('span', { \"class\" : \"toctext\"})\n",
    "\n",
    "#     if potential_section is not None:\n",
    "#         section = (''.join(potential_section.findAll(text=True))).strip()\n",
    "#         section_text[section] = \"\"\n",
    "#         current_section = section\n",
    "        \n",
    "#     else:\n",
    "#         text = (''.join(row.findAll(text=True))).strip()\n",
    "#         section_text[current_section] = section_text[current_section] + \" \" + text\n",
    "        \n",
    "# for key, value in section_text.items():\n",
    "#     print key\n",
    "#     print value\n",
    "#     print \"---------------------------------------------------------------------------------------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found...\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and shuffle order\n",
    "import collections\n",
    "from random import shuffle\n",
    "\n",
    "dataset = document_set.values()\n",
    "if len([item for item, count in collections.Counter(dataset).items() if count > 1]):\n",
    "    print \"Duplicates found...\"\n",
    "    dataset = list(set(dataset))\n",
    "    shuffle(dataset)\n",
    "else:\n",
    "    shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Custom tokenizer\n",
    "class tokenize_custom(object):\n",
    "    def __call__(self, doc):\n",
    "        # First tokenize by sentence then by word\n",
    "        tokens = [word for sent in nltk.sent_tokenize(doc) for word in nltk.word_tokenize(sent)]\n",
    "        \n",
    "        filtered_tokens = []\n",
    "        # Filter out any tokens not containing letters (numeric tokens, raw punctuation)\n",
    "        for token in tokens:\n",
    "            if len(token) > 3 and re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)                \n",
    "        return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 14.797s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 14.723s.\n",
      "Fitting LDA\n",
      "done in 58.448s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "distribution probability function distributions random parameter variables prior mean parameters variable likelihood given value beta\n",
      "Topic #1:\n",
      "hypothesis test null number example condorcet false positive precision candidate order result used error power\n",
      "Topic #2:\n",
      "algorithm data problem algorithms used method clustering compression time pagerank methods solution cluster number optimal\n",
      "Topic #3:\n",
      "search words document documents word used language information text verbs example index term terms relevant\n",
      "Topic #4:\n",
      "data model models regression learning used training decision loss analysis linear variables statistical using logistic\n",
      "Topic #5:\n",
      "entropy standard information mean deviation variance sample distribution population delta used time value given measure\n",
      "Topic #6:\n",
      "gini income hash table used coefficient number values index query time example bits value cost\n",
      "Topic #7:\n",
      "function matrix space real functions numbers point points complex product theorem vector called defined linear\n",
      "Topic #8:\n",
      "process quantum point poisson theory field state states space number mathematical intelligence classical processes measure\n",
      "Topic #9:\n",
      "number prime numbers integers primes integer theorem conjecture population polynomials polynomial values birthday given probability\n",
      "\n",
      "Fitting NMF\n",
      "done in 1.254s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "distribution probability random distributions function parameter prior variables variable binomial beta dirichlet cumulative posterior gamma\n",
      "Topic #1:\n",
      "function loss functions complex derivative exponential series gamma integral differentiable real theorem riemann sigmoid zeta\n",
      "Topic #2:\n",
      "prime primes conjecture integers polynomials numbers number polynomial twin irreducible theorem integer positive fibonacci ring\n",
      "Topic #3:\n",
      "document documents query search retrieval relevant recall text precision words word term queries information index\n",
      "Topic #4:\n",
      "quantum information state entropy classical qubit states qubits measurement bits theory neumann field alice annealing\n",
      "Topic #5:\n",
      "deviation standard sample mean variance population error normal estimator estimate deviations size unbiased samples square\n",
      "Topic #6:\n",
      "tobit model censored variable type tobin regression models james heckman latent econometrics likelihood censoring hours\n",
      "Topic #7:\n",
      "hypothesis null test power false sample tests variances effect chi-square statistical error testing significance t-test\n",
      "Topic #8:\n",
      "verbs verb auxiliary modal english auxiliaries word noun nouns adjectives speech languages main modality words\n",
      "Topic #9:\n",
      "data algorithm matrix model learning training problem vector method regression used linear space algorithms models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "n_topics = 10\n",
    "n_top_words = 15\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.9, min_df=2, tokenizer = tokenize_custom(), stop_words = 'english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(dataset)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, tokenizer = tokenize_custom(), stop_words = 'english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting LDA\")\n",
    "t0 = time()\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=100, learning_method='batch', random_state=1)\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting NMF\")\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "exit()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
