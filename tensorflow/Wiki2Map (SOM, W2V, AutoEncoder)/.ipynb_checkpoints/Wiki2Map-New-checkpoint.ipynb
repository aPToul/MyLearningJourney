{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bleach\n",
    "import requests\n",
    "\n",
    "# Wikipedia scraper from\n",
    "# http://www.gyford.com/phil/writing/2015/03/25/wikipedia-parsing.php\n",
    "\n",
    "class WikipediaFetcher(object):\n",
    "\n",
    "    def fetch(self, page_name):\n",
    "        \"\"\"\n",
    "        Passed a Wikipedia page's URL fragment, like\n",
    "        'Edward_Montagu,_1st_Earl_of_Sandwich', this will fetch the page's\n",
    "        main contents, tidy the HTML, strip out any elements we don't want\n",
    "        and return the final HTML string.\n",
    "\n",
    "        Returns a dict with two elements:\n",
    "            'success' is either True or, if we couldn't fetch the page, False.\n",
    "            'content' is the HTML if success==True, or else an error message.\n",
    "        \"\"\"\n",
    "        result = self._get_html(page_name)\n",
    "        \n",
    "        if result['success']:\n",
    "            result['content'] = self._tidy_html(result['content'])\n",
    "            \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def _get_html(self, page_name):\n",
    "        \"\"\"\n",
    "        Passed the name of a Wikipedia page (eg, 'Samuel_Pepys'), it fetches\n",
    "        the HTML content (not the entire HTML page) and returns it.\n",
    "\n",
    "        Returns a dict with two elements:\n",
    "            'success' is either True or, if we couldn't fetch the page, False.\n",
    "            'content' is the HTML if success==True, or else an error message.\n",
    "        \"\"\"\n",
    "        error_message = ''\n",
    "\n",
    "        url = 'https:%s' % page_name\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params={'action':'render'}, timeout=5)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            error_message = \"Can't connect to domain.\"\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            error_message = \"Connection timed out.\"\n",
    "        except requests.exceptions.TooManyRedirects as e:\n",
    "            error_message = \"Too many redirects.\"\n",
    "\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # 4xx or 5xx errors:\n",
    "            error_message = \"HTTP Error: %s\" % response.status_code\n",
    "        except NameError:\n",
    "            if error_message == '':\n",
    "                error_message = \"Something unusual went wrong.\"\n",
    "\n",
    "        if error_message:\n",
    "            return {'success': False, 'content': error_message} \n",
    "        else:\n",
    "            return {'success': True, 'content': response.text}\n",
    "\n",
    "    def _tidy_html(self, html):\n",
    "        \"\"\"\n",
    "        Passed the raw Wikipedia HTML, this returns valid HTML, with all\n",
    "        disallowed elements stripped out.\n",
    "        \"\"\"\n",
    "        #html = self._bleach_html(html)\n",
    "        #html = self._strip_html(html)\n",
    "        return html\n",
    "\n",
    "    def _bleach_html(self, html):\n",
    "        \"\"\"\n",
    "        Ensures we have valid HTML; no unclosed or mis-nested tags.\n",
    "        Removes any tags and attributes we don't want to let through.\n",
    "        Doesn't remove the contents of any disallowed tags.\n",
    "\n",
    "        Pass it an HTML string, it'll return the bleached HTML string.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pretty much most elements, but no forms or audio/video.\n",
    "        allowed_tags = [\n",
    "            'a', 'abbr', 'acronym', 'address', 'area', 'article',\n",
    "            'b', 'blockquote', 'br',\n",
    "            'caption', 'cite', 'code', 'col', 'colgroup',\n",
    "            'dd', 'del', 'dfn', 'div', 'dl', 'dt',\n",
    "            'em',\n",
    "            'figcaption', 'figure', 'footer',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'header', 'hgroup', 'hr',\n",
    "            'i', 'img', 'ins',\n",
    "            'kbd',\n",
    "            'li',\n",
    "            'map',\n",
    "            'nav',\n",
    "            'ol',\n",
    "            'p', 'pre',\n",
    "            'q',\n",
    "            's', 'samp', 'section', 'small', 'span', 'strong', 'sub', 'sup',\n",
    "            'table', 'tbody', 'td', 'tfoot', 'th', 'thead', 'time', 'tr',\n",
    "            'ul',\n",
    "            'var',\n",
    "        ]\n",
    "\n",
    "        # These attributes will be removed from any of the allowed tags.\n",
    "        allowed_attributes = {\n",
    "            '*':        ['class', 'id'],\n",
    "            'a':        ['href', 'title'],\n",
    "            'abbr':     ['title'],\n",
    "            'acronym':  ['title'],\n",
    "            'img':      ['alt', 'src', 'srcset'],\n",
    "            # Ugh. Don't know why this page doesn't use .tright like others\n",
    "            # http://127.0.0.1:8000/encyclopedia/5040/\n",
    "            'table':    ['align'],\n",
    "            'td':       ['colspan', 'rowspan'],\n",
    "            'th':       ['colspan', 'rowspan', 'scope'],\n",
    "        }\n",
    "\n",
    "        return bleach.clean(html, tags=allowed_tags,\n",
    "                                    attributes=allowed_attributes, strip=True)\n",
    "\n",
    "    def _strip_html(self, html):\n",
    "        \"\"\"\n",
    "        Takes out any tags, and their contents, that we don't want at all.\n",
    "        And adds custom classes to existing tags (so we can apply CSS styles\n",
    "        without having to multiply our CSS).\n",
    "\n",
    "        Pass it an HTML string, it returns the stripped HTML string.\n",
    "        \"\"\"\n",
    "\n",
    "        # CSS selectors. Strip these and their contents.\n",
    "        selectors = [\n",
    "            'div.hatnote',\n",
    "            'div.navbar.mini', # Will also match div.mini.navbar\n",
    "            # Bottom of https://en.wikipedia.org/wiki/Charles_II_of_England :\n",
    "            'div.topicon',\n",
    "            'a.mw-headline-anchor',\n",
    "        ]\n",
    "\n",
    "        # Strip any element that has one of these classes.\n",
    "        classes = [\n",
    "            # \"This article may be expanded with text translated from...\"\n",
    "            # https://en.wikipedia.org/wiki/Afonso_VI_of_Portugal\n",
    "            'ambox-notice',\n",
    "            'magnify',\n",
    "            # eg audio on https://en.wikipedia.org/wiki/Bagpipes\n",
    "            'mediaContainer',\n",
    "            'navbox',\n",
    "            'noprint',\n",
    "        ]\n",
    "\n",
    "        # Any element has a class matching a key, it will have the classes\n",
    "        # in the value added.\n",
    "        add_classes = {\n",
    "            # Give these tables standard Bootstrap styles.\n",
    "            'infobox':   ['table', 'table-bordered'],\n",
    "            'ambox':     ['table', 'table-bordered'],\n",
    "            'wikitable': ['table', 'table-bordered'],\n",
    "        } \n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        for selector in selectors:\n",
    "            [tag.decompose() for tag in soup.select(selector)]\n",
    "\n",
    "        for clss in classes:\n",
    "            [tag.decompose() for tag in soup.find_all(attrs={'class':clss})]\n",
    "\n",
    "        for clss, new_classes in add_classes.iteritems():\n",
    "            for tag in soup.find_all(attrs={'class':clss}):\n",
    "                tag['class'] = tag.get('class', []) + new_classes\n",
    "\n",
    "        # Depending on the HTML parser BeautifulSoup used, soup may have\n",
    "        # surrounding <html><body></body></html> or just <body></body> tags.\n",
    "        if soup.body:\n",
    "            soup = soup.body\n",
    "        elif soup.html:\n",
    "            soup = soup.html.body\n",
    "\n",
    "        # Put the content back into a string.\n",
    "        html = ''.join(str(tag) for tag in soup.contents)\n",
    "\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myScraper = WikipediaFetcher()\n",
    "\n",
    "original_sources = [\"//en.wikipedia.org/wiki/List_of_machine_learning_concepts\"\n",
    "                    ,\"//en.wikipedia.org/wiki/Data_mining\"\n",
    "                    ,\"//en.wikipedia.org/wiki/Machine_learning\"]\n",
    "\n",
    "links = []\n",
    "for page in original_sources:\n",
    "    scraped_article = myScraper.fetch(page_name=page)\n",
    "    if scraped_article['success'] == True:\n",
    "        soup = BeautifulSoup(scraped_article['content'], 'html.parser')\n",
    "\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            links.append(a['href'])\n",
    "    else:\n",
    "        print(\"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//en.wikipedia.org/wiki/Theoretical_computer_science\n",
      "//en.wikipedia.org/wiki/Operational_data_store\n",
      "//en.wikipedia.org/wiki/Security_service_(telecommunication)\n",
      "//en.wikipedia.org/wiki/DBSCAN\n",
      "//en.wikipedia.org/wiki/AODE\n",
      "//en.wikipedia.org/wiki/Deeplearning4j\n",
      "//en.wikipedia.org/wiki/Analysis_of_algorithms\n",
      "//en.wikipedia.org/wiki/Robert_Tibshirani\n",
      "//en.wikipedia.org/wiki/Handwriting_recognition\n",
      "//en.wikipedia.org/wiki/Natural_selection\n",
      "//en.wikipedia.org/wiki/Software_configuration_management\n",
      "//en.wikipedia.org/wiki/Cognitive_model\n",
      "//en.wikipedia.org/wiki/Hidden_Markov_model\n",
      "//en.wikipedia.org/wiki/Machine_ethics\n",
      "//en.wikipedia.org/wiki/Tanagra_(machine_learning)\n",
      "//en.wikipedia.org/wiki/Intrusion_detection_system\n",
      "//en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
      "//en.wikipedia.org/wiki/Mutation_(genetic_algorithm)\n",
      "//en.wikipedia.org/wiki/Information_extraction\n",
      "//en.wikipedia.org/wiki/Business_intelligence\n",
      "//en.wikipedia.org/wiki/Q-learning\n",
      "//en.wikipedia.org/wiki/Nearest_neighbor_(pattern_recognition)\n",
      "//en.wikipedia.org/wiki/Learning_Automata\n",
      "//en.wikipedia.org/wiki/Interpreter_(computing)\n",
      "//en.wikipedia.org/wiki/Vector_Quantization\n",
      "//en.wikipedia.org/wiki/Computer_data_storage\n",
      "//en.wikipedia.org/wiki/Regression_analysis\n",
      "//en.wikipedia.org/wiki/Expectation-maximization_algorithm\n",
      "//en.wikipedia.org/wiki/Digital_object_identifier\n",
      "//en.wikipedia.org/wiki/National_Diet_Library\n",
      "//en.wikipedia.org/wiki/Spike-and-slab_variable_selection\n",
      "//en.wikipedia.org/wiki/Numerical_analysis\n",
      "//en.wikipedia.org/wiki/Data_transformation\n",
      "//en.wikipedia.org/wiki/Expert_system\n",
      "//en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research\n",
      "//en.wikipedia.org/wiki/Jerome_H._Friedman\n",
      "//en.wikipedia.org/wiki/National_Security_Agency\n",
      "//en.wikipedia.org/wiki/Limitations_and_exceptions_to_copyright\n",
      "//en.wikipedia.org/wiki/Automatic_summarization\n",
      "//en.wikipedia.org/wiki/SPSS_Modeler\n",
      "//en.wikipedia.org/wiki/Credit_card_fraud\n",
      "//en.wikipedia.org/wiki/Recurrent_neural_network\n",
      "//en.wikipedia.org/wiki/Recommendation_systems\n",
      "//en.wikipedia.org/wiki/Gene_expression_programming\n",
      "//en.wikipedia.org/wiki/Geographic_information_system\n",
      "//en.wikipedia.org/wiki/Rendering_(computer_graphics)\n",
      "//en.wikipedia.org/wiki/Computer_graphics\n",
      "//en.wikipedia.org/wiki/Ensembles_of_classifiers\n",
      "//en.wikipedia.org/wiki/Speech_recognition\n",
      "//en.wikipedia.org/wiki/Category:CS1_maint:_Multiple_names:_authors_list\n",
      "//en.wikipedia.org/wiki/Qlucore\n",
      "//en.wikipedia.org/wiki/Instance-based_learning\n",
      "//en.wikipedia.org/wiki/Automated_planning_and_scheduling\n",
      "//en.wikipedia.org/wiki/Data_mining_in_meteorology\n",
      "//en.wikipedia.org/wiki/Database\n",
      "//en.wikipedia.org/wiki/Social_computing\n",
      "//en.wikipedia.org/wiki/Single-linkage_clustering\n",
      "//en.wikipedia.org/wiki/Bayesian_statistics\n",
      "//en.wikipedia.org/wiki/K-SVD\n",
      "//en.wikipedia.org/wiki/Probability_theory\n",
      "//en.wikipedia.org/wiki/Arthur_Samuel\n",
      "//en.wikipedia.org/wiki/Electronic_discovery\n",
      "//en.wikipedia.org/wiki/Information_integration\n",
      "//en.wikipedia.org/wiki/Process_control\n",
      "//en.wikipedia.org/wiki/Software_construction\n",
      "//en.wikipedia.org/wiki/Dimension_table\n",
      "//en.wikipedia.org/wiki/ELKI\n",
      "//en.wikipedia.org/wiki/Argonne_National_Laboratory\n",
      "//en.wikipedia.org/wiki/Multiprocessing\n",
      "//en.wikipedia.org/wiki/Computing_platform\n",
      "//en.wikipedia.org/wiki/List_of_machine_learning_concepts\n",
      "//en.wikipedia.org/wiki/Non-linear\n",
      "//en.wikipedia.org/wiki/Library_(computing)\n",
      "//en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research\n",
      "//en.wikipedia.org/wiki/Adaptive_control\n",
      "//en.wikipedia.org/wiki/Statistical_noise\n",
      "//en.wikipedia.org/wiki/Rexer%27s_Annual_Data_Miner_Survey\n",
      "//en.wikipedia.org/wiki/GNU_Octave\n",
      "//en.wikipedia.org/wiki/Interaction_design\n",
      "//en.wikipedia.org/wiki/K-nearest_neighbors_classification\n",
      "//en.wikipedia.org/wiki/Theory_of_computation\n",
      "//en.wikipedia.org/wiki/Support_vector_machine\n",
      "//en.wikipedia.org/wiki/Computer_animation\n",
      "//en.wikipedia.org/wiki/R_(programming_language)\n",
      "//en.wikipedia.org/wiki/Slowly_changing_dimension\n",
      "//en.wikipedia.org/wiki/Online_algorithm\n",
      "//en.wikipedia.org/wiki/XML_for_Analysis\n",
      "//en.wikipedia.org/wiki/InformationWeek\n",
      "//en.wikipedia.org/wiki/Distributed_computing\n",
      "//en.wikipedia.org/wiki/Data_Vault_Modeling\n",
      "//en.wikipedia.org/wiki/Computational_statistics\n",
      "//en.wikipedia.org/wiki/Machine_perception\n",
      "//en.wikipedia.org/wiki/Anomaly_detection\n",
      "//en.wikipedia.org/wiki/Metaheuristic\n",
      "//en.wikipedia.org/wiki/Pedro_Domingos\n",
      "//en.wikipedia.org/wiki/Search_engines\n",
      "//en.wikipedia.org/wiki/Boosting_(meta-algorithm)\n",
      "//en.wikipedia.org/wiki/Software_engineering\n",
      "//en.wikipedia.org/wiki/Information_Fuzzy_Networks\n",
      "//en.wikipedia.org/wiki/Grammar_induction\n",
      "//en.wikipedia.org/wiki/Natural_Language_Toolkit\n",
      "//en.wikipedia.org/wiki/Chromosome_(genetic_algorithm)\n",
      "//en.wikipedia.org/wiki/Decision_rules\n",
      "//en.wikipedia.org/wiki/Factor_analysis\n",
      "//en.wikipedia.org/wiki/Multi-label_classification\n",
      "//en.wikipedia.org/wiki/Orange_(software)\n",
      "//en.wikipedia.org/wiki/European_Conference_on_Machine_Learning_and_Principles_and_Practice_of_Knowledge_Discovery_in_Databases\n",
      "//en.wikipedia.org/wiki/Lua_(programming_language)\n",
      "//en.wikipedia.org/wiki/K-means_algorithm\n",
      "//en.wikipedia.org/wiki/AAAI\n",
      "//en.wikipedia.org/wiki/Integrated_circuit\n",
      "//en.wikipedia.org/wiki/Syntactic_pattern_recognition\n",
      "//en.wikipedia.org/wiki/Virtual_machine\n",
      "//en.wikipedia.org/wiki/Shogun_(toolbox)\n",
      "//en.wikipedia.org/wiki/IEEE_Signal_Processing_Society\n",
      "//en.wikipedia.org/wiki/Programming_language\n",
      "//en.wikipedia.org/wiki/Cross-platform\n",
      "//en.wikipedia.org/wiki/Wikipedia:Citation_needed\n",
      "//en.wikipedia.org/wiki/Data_modeling\n",
      "//en.wikipedia.org/wiki/Test_set\n",
      "//en.wikipedia.org/wiki/Integrated_development_environment\n",
      "//en.wikipedia.org/wiki/Directed_acyclic_graph\n",
      "//en.wikipedia.org/wiki/Total_Information_Awareness\n",
      "//en.wikipedia.org/wiki/Learning_to_rank\n",
      "//en.wikipedia.org/wiki/Computational_complexity_theory\n",
      "//en.wikipedia.org/wiki/Stanford_University\n",
      "//en.wikipedia.org/wiki/Agent_mining\n",
      "//en.wikipedia.org/wiki/Neural_Computation\n",
      "//en.wikipedia.org/wiki/Temporal_difference_learning\n",
      "//en.wikipedia.org/wiki/Star_schema\n",
      "//en.wikipedia.org/wiki/Information_bottleneck_method\n",
      "//en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence\n",
      "//en.wikipedia.org/wiki/Bill_Inmon\n",
      "//en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      "//en.wikipedia.org/wiki/Mathematical_analysis\n",
      "//en.wikipedia.org/wiki/Data_pre-processing\n",
      "//en.wikipedia.org/wiki/Computational_biology\n",
      "//en.wikipedia.org/wiki/Machine_learning\n",
      "//en.wikipedia.org/wiki/ANOVA\n",
      "//en.wikipedia.org/wiki/Statistical_inference\n",
      "//en.wikipedia.org/wiki/Logic_in_computer_science\n",
      "//en.wikipedia.org/wiki/Neural_network\n",
      "//en.wikipedia.org/wiki/Multilayer_perceptron\n",
      "//en.wikipedia.org/wiki/Yooreeka\n",
      "//en.wikipedia.org/wiki/Real-time_computing\n",
      "//en.wikipedia.org/wiki/Genetic_algorithm\n",
      "//en.wikipedia.org/wiki/Diagnosis_(artificial_intelligence)\n",
      "//en.wikipedia.org/wiki/Dimensional_modeling\n",
      "//en.wikipedia.org/wiki/Bias-variance_dilemma\n",
      "//en.wikipedia.org/wiki/Autoencoder\n",
      "//en.wikipedia.org/wiki/Computer_science\n",
      "//en.wikipedia.org/wiki/Internet\n",
      "//en.wikipedia.org/wiki/Spatial_index\n",
      "//en.wikipedia.org/wiki/Template:Data_warehouse\n",
      "//en.wikipedia.org/wiki/Portal:Computer_science\n",
      "//en.wikipedia.org/wiki/UIMA\n",
      "//en.wikipedia.org/wiki/Profiling_(information_science)\n",
      "//en.wikipedia.org/wiki/Photo_manipulation\n",
      "//en.wikipedia.org/wiki/Data_set\n",
      "//en.wikipedia.org/wiki/Statistics\n",
      "//en.wikipedia.org/wiki/Topic_modeling\n",
      "//en.wikipedia.org/wiki/Ray_Solomonoff\n",
      "//en.wikipedia.org/wiki/Cambridge_University_Press\n",
      "//en.wikipedia.org/wiki/John_Hopfield\n",
      "//en.wikipedia.org/wiki/State-Action-Reward-State-Action\n",
      "//en.wikipedia.org/wiki/Kluwer_Academic_Publishers\n",
      "//en.wikipedia.org/wiki/Probably_approximately_correct_learning\n",
      "//en.wikipedia.org/wiki/Information_theory\n",
      "//en.wikipedia.org/wiki/Prentice_Hall\n",
      "//en.wikipedia.org/wiki/Local_outlier_factor\n",
      "//en.wikipedia.org/wiki/Data_integration\n",
      "//en.wikipedia.org/wiki/Information_security\n",
      "//en.wikipedia.org/wiki/Logistic_Model_Tree\n",
      "//en.wikipedia.org/wiki/Sparse_dictionary_learning\n",
      "//en.wikipedia.org/wiki/Principal_components_analysis\n",
      "//en.wikipedia.org/wiki/SIGKDD\n",
      "//en.wikipedia.org/wiki/European_Commission\n",
      "//en.wikipedia.org/wiki/Extract,_transform,_load\n",
      "//en.wikipedia.org/wiki/Automata_theory\n",
      "//en.wikipedia.org/wiki/Artificial_neuron\n",
      "//en.wikipedia.org/wiki/Early-arriving_fact\n",
      "//en.wikipedia.org/wiki/Stock_market\n",
      "//en.wikipedia.org/wiki/Computational_chemistry\n",
      "//en.wikipedia.org/wiki/Ripple_down_rules\n",
      "//en.wikipedia.org/wiki/Computer_security_compromised_by_hardware_failure\n",
      "//en.wikipedia.org/wiki/Machine_Learning_(journal)\n",
      "//en.wikipedia.org/wiki/Software_suite\n",
      "//en.wikipedia.org/wiki/The_American_Statistician\n",
      "//en.wikipedia.org/wiki/ADALINE\n",
      "//en.wikipedia.org/wiki/Ensemble_Averaging\n",
      "//en.wikipedia.org/wiki/Linear_regression\n",
      "//en.wikipedia.org/wiki/Affective_computing\n",
      "//en.wikipedia.org/wiki/CIKM_Conference\n",
      "//en.wikipedia.org/wiki/Social_Science_Research_Network\n",
      "//en.wikipedia.org/wiki/Digital_art\n",
      "//en.wikipedia.org/wiki/Lazy_learning\n",
      "//en.wikipedia.org/wiki/H2o_(Analytics_tool)\n",
      "//en.wikipedia.org/wiki/Peter_Norvig\n",
      "//en.wikipedia.org/wiki/Gregory_I._Piatetsky-Shapiro\n",
      "//en.wikipedia.org/wiki/MLPACK_(C%2B%2B_library)\n",
      "//en.wikipedia.org/wiki/Recommender_system\n",
      "//en.wikipedia.org/wiki/Middleware\n",
      "//en.wikipedia.org/wiki/Global_surveillance_disclosure\n",
      "//en.wikipedia.org/wiki/Probability\n",
      "//en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\n",
      "//en.wikipedia.org/wiki/KDD_Conference\n",
      "//en.wikipedia.org/wiki/Digital_library\n",
      "//en.wikipedia.org/wiki/Printed_circuit_board\n",
      "//en.wikipedia.org/wiki/Structured_prediction\n",
      "//en.wikipedia.org/wiki/Joint_probability_distribution\n",
      "//en.wikipedia.org/wiki/Application_security\n",
      "//en.wikipedia.org/wiki/E-commerce\n",
      "//en.wikipedia.org/wiki/Multi-task_learning\n",
      "//en.wikipedia.org/wiki/Michael_I._Jordan\n",
      "//en.wikipedia.org/wiki/Computational_engineering\n",
      "//en.wikipedia.org/wiki/Computational_learning_theory\n",
      "//en.wikipedia.org/wiki/Perceptron\n",
      "//en.wikipedia.org/wiki/Bayes%27_theorem\n",
      "//en.wikipedia.org/wiki/Document_management_system\n",
      "//en.wikipedia.org/wiki/Boltzmann_machine\n",
      "//en.wikipedia.org/wiki/Quadratic_classifier\n",
      "//en.wikipedia.org/wiki/Conditional_random_field\n",
      "//en.wikipedia.org/wiki/Gaussian_process_regression\n",
      "//en.wikipedia.org/wiki/Java_(programming_language)\n",
      "//en.wikipedia.org/wiki/Biological_neural_networks\n",
      "//en.wikipedia.org/wiki/Canonical_correlation_analysis\n",
      "//en.wikipedia.org/wiki/SCaViS\n",
      "//en.wikipedia.org/wiki/Network_security\n",
      "//en.wikipedia.org/wiki/Electronic_design_automation\n",
      "//en.wikipedia.org/wiki/Computational_social_science\n",
      "//en.wikipedia.org/wiki/Manifold_learning\n",
      "//en.wikipedia.org/wiki/ACM_Computing_Classification_System\n",
      "//en.wikipedia.org/wiki/MIT_Press\n",
      "//en.wikipedia.org/wiki/Web_mining\n",
      "//en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory\n",
      "//en.wikipedia.org/wiki/Network_protocol\n",
      "//en.wikipedia.org/wiki/Data_extraction\n",
      "//en.wikipedia.org/wiki/Software_development_process\n",
      "//en.wikipedia.org/wiki/Named-entity_recognition\n",
      "//en.wikipedia.org/wiki/David_J._C._MacKay\n",
      "//en.wikipedia.org/wiki/MOA_(Massive_Online_Analysis)\n",
      "//en.wikipedia.org/wiki/Counterpunch.org\n",
      "//en.wikipedia.org/wiki/Enterprise_information_system\n",
      "//en.wikipedia.org/wiki/Supervised_learning\n",
      "//en.wikipedia.org/wiki/The_Master_Algorithm\n",
      "//en.wikipedia.org/wiki/Cache_language_model\n",
      "//en.wikipedia.org/wiki/Cyberwarfare\n",
      "//en.wikipedia.org/wiki/File:Svm_max_sep_hyperplane_with_margin.png\n",
      "//en.wikipedia.org/wiki/Template_talk:Machine_learning_bar\n",
      "//en.wikipedia.org/wiki/NeuroSolutions\n",
      "//en.wikipedia.org/wiki/Deep_learning\n",
      "//en.wikipedia.org/wiki/NLTK\n",
      "//en.wikipedia.org/wiki/Discrete_mathematics\n",
      "//en.wikipedia.org/wiki/Jiawei_Han\n",
      "//en.wikipedia.org/wiki/ADVISE\n",
      "//en.wikipedia.org/wiki/Map_(mathematics)\n",
      "//en.wikipedia.org/wiki/Behavior_informatics\n",
      "//en.wikipedia.org/wiki/Strategy_game\n",
      "//en.wikipedia.org/wiki/Software_framework\n",
      "//en.wikipedia.org/wiki/IBM\n",
      "//en.wikipedia.org/wiki/Netflix_Prize\n",
      "//en.wikipedia.org/wiki/Linear_classifier\n",
      "//en.wikipedia.org/wiki/Big_data\n",
      "//en.wikipedia.org/wiki/Fisher%27s_linear_discriminant\n",
      "//en.wikipedia.org/wiki/Mathematica\n",
      "//en.wikipedia.org/wiki/Edward_Snowden\n",
      "//en.wikipedia.org/wiki/ECML_PKDD\n",
      "//en.wikipedia.org/wiki/Reproducibility\n",
      "//en.wikipedia.org/wiki/Python_(programming_language)\n",
      "//en.wikipedia.org/wiki/Mallet_(software_project)\n",
      "//en.wikipedia.org/wiki/Programming_paradigm\n",
      "//en.wikipedia.org/wiki/Training_set\n",
      "//en.wikipedia.org/wiki/Inductive_logic_programming\n",
      "//en.wikipedia.org/wiki/Philip_S._Yu\n",
      "//en.wikipedia.org/wiki/Database_Directive\n",
      "//en.wikipedia.org/wiki/Connectionism\n",
      "//en.wikipedia.org/wiki/Multinomial_logistic_regression\n",
      "//en.wikipedia.org/wiki/KXEN_Inc.\n",
      "//en.wikipedia.org/wiki/Mathematical_optimization\n",
      "//en.wikipedia.org/wiki/Copyright_Directive\n",
      "//en.wikipedia.org/wiki/Peripheral\n",
      "//en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining\n",
      "//en.wikipedia.org/wiki/Apache_Spark\n",
      "//en.wikipedia.org/wiki/Glossary_of_artificial_intelligence\n",
      "//en.wikipedia.org/wiki/Control_theory\n",
      "//en.wikipedia.org/wiki/Domain-specific_language\n",
      "//en.wikipedia.org/wiki/Online_machine_learning\n",
      "//en.wikipedia.org/wiki/Leo_Breiman\n",
      "//en.wikipedia.org/wiki/Fuzzy_clustering\n",
      "//en.wikipedia.org/wiki/Computational_mathematics\n",
      "//en.wikipedia.org/wiki/Conflate\n",
      "//en.wikipedia.org/wiki/Category:Data_mining_and_machine_learning_software\n",
      "//en.wikipedia.org/wiki/Case-based_reasoning\n",
      "//en.wikipedia.org/wiki/Database_management_system\n",
      "//en.wikipedia.org/wiki/Structural_health_monitoring\n",
      "//en.wikipedia.org/wiki/Self-organizing_map\n",
      "//en.wikipedia.org/wiki/Empirical_risk_minimization\n",
      "//en.wikipedia.org/wiki/Electronic_voting\n",
      "//en.wikipedia.org/wiki/Computation\n",
      "//en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
      "//en.wikipedia.org/wiki/NetOwl\n",
      "//en.wikipedia.org/wiki/BIRCH\n",
      "//en.wikipedia.org/wiki/Independent_component_analysis\n",
      "//en.wikipedia.org/wiki/Explanation-based_learning\n",
      "//en.wikipedia.org/wiki/Java_Data_Mining\n",
      "//en.wikipedia.org/wiki/Eclat_algorithm\n",
      "//en.wikipedia.org/wiki/Society_for_Industrial_and_Applied_Mathematics\n",
      "//en.wikipedia.org/wiki/Similarity_learning\n",
      "//en.wikipedia.org/wiki/C4.5\n",
      "//en.wikipedia.org/wiki/Drug_discovery\n",
      "//en.wikipedia.org/wiki/ROLAP\n",
      "//en.wikipedia.org/wiki/Open_access\n",
      "//en.wikipedia.org/wiki/Programming_language_theory\n",
      "//en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
      "//en.wikipedia.org/wiki/Educational_technology\n",
      "//en.wikipedia.org/wiki/XML\n",
      "//en.wikipedia.org/wiki/Business_intelligence_tools\n",
      "//en.wikipedia.org/wiki/Domain_driven_data_mining\n",
      "//en.wikipedia.org/wiki/Alan_Turing\n",
      "//en.wikipedia.org/wiki/Non-negative_matrix_factorization\n",
      "//en.wikipedia.org/wiki/RCASE\n",
      "//en.wikipedia.org/wiki/Conference_on_Knowledge_Discovery_and_Data_Mining\n",
      "//en.wikipedia.org/wiki/Subspace_clustering\n",
      "//en.wikipedia.org/wiki/Logistic_regression\n",
      "//en.wikipedia.org/wiki/Bioinformatics\n",
      "//en.wikipedia.org/wiki/Adversarial_machine_learning\n",
      "//en.wikipedia.org/wiki/Search_algorithm\n",
      "//en.wikipedia.org/wiki/Association_for_Computing_Machinery\n",
      "//en.wikipedia.org/wiki/Predictive_modelling\n",
      "//en.wikipedia.org/wiki/Data_mining_in_agriculture\n",
      "//en.wikipedia.org/wiki/Robot_locomotion\n",
      "//en.wikipedia.org/wiki/Computer_vision\n",
      "//en.wikipedia.org/wiki/Data_dictionary\n",
      "//en.wikipedia.org/wiki/Software_repository\n",
      "//en.wikipedia.org/wiki/Meta_learning_(computer_science)\n",
      "//en.wikipedia.org/wiki/Stellar_Wind_(code_name)\n",
      "//en.wikipedia.org/wiki/Overfitting\n",
      "//en.wikipedia.org/wiki/Backpropagation\n",
      "//en.wikipedia.org/wiki/Data_Pre-processing\n",
      "//en.wikipedia.org/wiki/Ensemble_learning\n",
      "//en.wikipedia.org/wiki/AT%26T_Labs\n",
      "//en.wikipedia.org/wiki/Unsupervised_learning\n",
      "//en.wikipedia.org/wiki/Requirements_analysis\n",
      "//en.wikipedia.org/wiki/Computer_network\n",
      "//en.wikipedia.org/wiki/Feature_engineering\n",
      "//en.wikipedia.org/wiki/Encog\n",
      "//en.wikipedia.org/wiki/Fact_table\n",
      "//en.wikipedia.org/wiki/Generative_topographic_map\n",
      "//en.wikipedia.org/wiki/Programming_tool\n",
      "//en.wikipedia.org/wiki/Cheminformatics\n",
      "//en.wikipedia.org/wiki/Graphical_model\n",
      "//en.wikipedia.org/wiki/File:Animation2.gif\n",
      "//en.wikipedia.org/wiki/Network_architecture\n",
      "//en.wikipedia.org/wiki/Gregory_Piatetsky-Shapiro\n",
      "//en.wikipedia.org/wiki/Compiler_construction\n",
      "//en.wikipedia.org/wiki/OpenNN\n",
      "//en.wikipedia.org/wiki/Comparison_of_OLAP_Servers\n",
      "//en.wikipedia.org/wiki/Netflix\n",
      "//en.wikipedia.org/wiki/Transduction_(machine_learning)\n",
      "//en.wikipedia.org/wiki/Network_performance\n",
      "//en.wikipedia.org/wiki/Predictive_Model_Markup_Language\n",
      "//en.wikipedia.org/wiki/Mathematical_model\n",
      "//en.wikipedia.org/wiki/Representation_learning\n",
      "//en.wikipedia.org/wiki/Scikit-image\n",
      "//en.wikipedia.org/wiki/Fact_(data_warehouse)\n",
      "//en.wikipedia.org/wiki/Cryptography\n",
      "//en.wikipedia.org/wiki/Object_recognition\n",
      "//en.wikipedia.org/wiki/Inference\n",
      "//en.wikipedia.org/wiki/Decision_tree\n",
      "//en.wikipedia.org/wiki/Semi-supervised_learning\n",
      "//en.wikipedia.org/wiki/Data_warehouse\n",
      "//en.wikipedia.org/wiki/Learning_Vector_Quantization\n",
      "//en.wikipedia.org/wiki/Model_of_computation\n",
      "//en.wikipedia.org/wiki/Educational_data_mining\n",
      "//en.wikipedia.org/wiki/Concurrency_(computer_science)\n",
      "//en.wikipedia.org/wiki/Learning\n",
      "//en.wikipedia.org/wiki/Operating_system\n",
      "//en.wikipedia.org/wiki/Family_Educational_Rights_and_Privacy_Act\n",
      "//en.wikipedia.org/wiki/Inductive_programming\n",
      "//en.wikipedia.org/wiki/Data_loading\n",
      "//en.wikipedia.org/wiki/Intention_mining\n",
      "//en.wikipedia.org/wiki/Neural_Designer\n",
      "//en.wikipedia.org/wiki/Examples_of_data_mining\n",
      "//en.wikipedia.org/wiki/Category:Data_warehousing_products\n",
      "//en.wikipedia.org/wiki/Statistical_hypothesis_testing\n",
      "//en.wikipedia.org/wiki/Metadata\n",
      "//en.wikipedia.org/wiki/Artificial_neural_network\n",
      "//en.wikipedia.org/wiki/Template:Computer_science\n",
      "//en.wikipedia.org/wiki/Ryszard_S._Michalski\n",
      "//en.wikipedia.org/wiki/Natural_language\n",
      "//en.wikipedia.org/wiki/C%2B%2B\n",
      "//en.wikipedia.org/wiki/Probability_distribution\n",
      "//en.wikipedia.org/wiki/Computational_anatomy\n",
      "//en.wikipedia.org/wiki/Inductive_bias\n",
      "//en.wikipedia.org/wiki/Economics\n",
      "//en.wikipedia.org/wiki/Principal_component_analysis\n",
      "//en.wikipedia.org/wiki/Google_Scholar\n",
      "//en.wikipedia.org/wiki/Neural_networks\n",
      "//en.wikipedia.org/wiki/SEMMA\n",
      "//en.wikipedia.org/wiki/Knowledge_discovery\n",
      "//en.wikipedia.org/wiki/Aggregate_function\n",
      "//en.wikipedia.org/wiki/List_of_reporting_software\n",
      "//en.wikipedia.org/wiki/Decision_tree_learning\n",
      "//en.wikipedia.org/wiki/Sequence_mining\n",
      "//en.wikipedia.org/wiki/Torch_(machine_learning)\n",
      "//en.wikipedia.org/wiki/Mlpy\n",
      "//en.wikipedia.org/wiki/Data_mart\n",
      "//en.wikipedia.org/wiki/Conditional_Random_Field\n",
      "//en.wikipedia.org/wiki/Mixed_reality\n",
      "//en.wikipedia.org/wiki/Data\n",
      "//en.wikipedia.org/wiki/Visualization_(computer_graphics)\n",
      "//en.wikipedia.org/wiki/Cluster_analysis\n",
      "//en.wikipedia.org/wiki/Association_rule_learning\n",
      "//en.wikipedia.org/wiki/Restricted_Boltzmann_Machine\n",
      "//en.wikipedia.org/wiki/Missing_data\n",
      "//en.wikipedia.org/wiki/Talk:Data_mining\n",
      "//en.wikipedia.org/wiki/Electronic_publishing\n",
      "//en.wikipedia.org/wiki/Dimension_(data_warehouse)\n",
      "//en.wikipedia.org/wiki/Clarabridge\n",
      "//en.wikipedia.org/wiki/Online_analytical_processing\n",
      "//en.wikipedia.org/wiki/Concurrent_computing\n",
      "//en.wikipedia.org/wiki/Software_design\n",
      "//en.wikipedia.org/wiki/Andrew_Ng\n",
      "//en.wikipedia.org/wiki/Dimensionality_reduction\n",
      "//en.wikipedia.org/wiki/Adaptive_website\n",
      "//en.wikipedia.org/wiki/Azure_machine_learning_studio\n",
      "//en.wikipedia.org/wiki/Computational_physics\n",
      "//en.wikipedia.org/wiki/Parallel_computing\n",
      "//en.wikipedia.org/wiki/Degenerate_dimension\n",
      "//en.wikipedia.org/wiki/Statistical_classification\n",
      "//en.wikipedia.org/wiki/Word_processor\n",
      "//en.wikipedia.org/wiki/Help:Authority_control\n",
      "//en.wikipedia.org/wiki/Association_rule_mining\n",
      "//en.wikipedia.org/wiki/International_Safe_Harbor_Privacy_Principles\n",
      "//en.wikipedia.org/wiki/Statistical_learning_theory\n",
      "//en.wikipedia.org/wiki/Springer_Verlag\n",
      "//en.wikipedia.org/wiki/Information_retrieval\n",
      "//en.wikipedia.org/wiki/Software_development\n",
      "//en.wikipedia.org/wiki/Academic_journal\n",
      "//en.wikipedia.org/wiki/Online_advertising\n",
      "//en.wikipedia.org/wiki/Ubiquitous_computing\n",
      "//en.wikipedia.org/wiki/RapidMiner\n",
      "//en.wikipedia.org/wiki/Distributed_artificial_intelligence\n",
      "//en.wikipedia.org/wiki/Scikit-learn\n",
      "//en.wikipedia.org/wiki/Big_Data\n",
      "//en.wikipedia.org/wiki/KNIME\n",
      "//en.wikipedia.org/wiki/Dlib\n",
      "//en.wikipedia.org/wiki/Data_Mining_and_Knowledge_Discovery\n",
      "//en.wikipedia.org/wiki/Text_mining\n",
      "//en.wikipedia.org/wiki/Portal:Artificial_intelligence\n",
      "//en.wikipedia.org/wiki/Natural_language_processing\n",
      "//en.wikipedia.org/wiki/Convolutional_neural_network\n",
      "//en.wikipedia.org/wiki/Chemicalize.org\n",
      "//en.wikipedia.org/wiki/Multivariate_statistics\n",
      "//en.wikipedia.org/wiki/Marketing\n",
      "//en.wikipedia.org/wiki/Feature_learning\n",
      "//en.wikipedia.org/wiki/Conference_on_Information_and_Knowledge_Management\n",
      "//en.wikipedia.org/wiki/Algorithm\n",
      "//en.wikipedia.org/wiki/Mean-shift\n",
      "//en.wikipedia.org/wiki/Buzzword\n",
      "//en.wikipedia.org/wiki/Fair_use\n",
      "//en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
      "//en.wikipedia.org/wiki/Robot_learning\n",
      "//en.wikipedia.org/wiki/Angoss\n",
      "//en.wikipedia.org/wiki/Usama_Fayyad\n",
      "//en.wikipedia.org/wiki/Scientific_computing\n",
      "//en.wikipedia.org/wiki/Strongly_NP-hard\n",
      "//en.wikipedia.org/wiki/Software_deployment\n",
      "//en.wikipedia.org/wiki/Logic_programming\n",
      "//en.wikipedia.org/wiki/Open-source_software\n",
      "//en.wikipedia.org/wiki/File:Internet_map_1024.jpg\n",
      "//en.wikipedia.org/wiki/LIONsolver\n",
      "//en.wikipedia.org/wiki/Ralph_Kimball\n",
      "//en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
      "//en.wikipedia.org/wiki/Heuristic_(computer_science)\n",
      "//en.wikipedia.org/wiki/Analogical_modeling\n",
      "//en.wikipedia.org/wiki/Dartmouth_Conferences\n",
      "//en.wikipedia.org/wiki/Software_maintenance\n",
      "//en.wikipedia.org/wiki/Image_compression\n",
      "//en.wikipedia.org/wiki/ML-Flex\n",
      "//en.wikipedia.org/wiki/Dependability\n",
      "//en.wikipedia.org/wiki/Predictive_analytics\n",
      "//en.wikipedia.org/wiki/Human%E2%80%93computer_interaction\n",
      "//en.wikipedia.org/wiki/Generalized_linear_model\n",
      "//en.wikipedia.org/wiki/Forrester_Research\n",
      "//en.wikipedia.org/wiki/Vertica\n",
      "//en.wikipedia.org/wiki/Analytics\n",
      "//en.wikipedia.org/wiki/Academic_Press\n",
      "//en.wikipedia.org/wiki/Support_vector_machines\n",
      "//en.wikipedia.org/wiki/Green_computing\n",
      "//en.wikipedia.org/wiki/Operations_research\n",
      "//en.wikipedia.org/wiki/OPTICS_algorithm\n",
      "//en.wikipedia.org/wiki/Group_method_of_data_handling\n",
      "//en.wikipedia.org/wiki/MATLAB\n",
      "//en.wikipedia.org/wiki/Solid_modeling\n",
      "//en.wikipedia.org/wiki/Randomized_algorithm\n",
      "//en.wikipedia.org/wiki/Relevance_vector_machine\n",
      "//en.wikipedia.org/wiki/ArXiv\n",
      "//en.wikipedia.org/wiki/Co-training\n",
      "//en.wikipedia.org/wiki/Random_forest\n",
      "//en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach\n",
      "//en.wikipedia.org/wiki/Computational_geometry\n",
      "//en.wikipedia.org/wiki/Exploratory_data_analysis\n",
      "//en.wikipedia.org/wiki/GNU_Project\n",
      "//en.wikipedia.org/wiki/Bayesian_network\n",
      "//en.wikipedia.org/wiki/Dashboard_(business)\n",
      "//en.wikipedia.org/wiki/Computer_accessibility\n",
      "//en.wikipedia.org/wiki/Bayesian_Structural_Time_Series\n",
      "//en.wikipedia.org/wiki/Computer_architecture\n",
      "//en.wikipedia.org/wiki/Developmental_robotics\n",
      "//en.wikipedia.org/wiki/Reinforcement_learning\n",
      "//en.wikipedia.org/wiki/Open_Text_Corporation\n",
      "//en.wikipedia.org/wiki/Template_talk:Computer_science\n",
      "//en.wikipedia.org/wiki/Bias%E2%80%93variance_decomposition\n",
      "//en.wikipedia.org/wiki/Open_source\n",
      "//en.wikipedia.org/wiki/SIGMOD\n",
      "//en.wikipedia.org/wiki/Ethics_of_artificial_intelligence\n",
      "//en.wikipedia.org/wiki/Measure_(data_warehouse)\n",
      "//en.wikipedia.org/wiki/Web_scraping\n",
      "//en.wikipedia.org/wiki/International_Conference_on_Very_Large_Data_Bases\n",
      "//en.wikipedia.org/wiki/Carrot2\n",
      "//en.wikipedia.org/wiki/Local_Outlier_Factor\n",
      "//en.wikipedia.org/wiki/List_of_machine_learning_algorithms\n",
      "//en.wikipedia.org/wiki/Hargreaves_review\n",
      "//en.wikipedia.org/wiki/Social_software\n",
      "//en.wikipedia.org/wiki/Computational_neuroscience\n",
      "//en.wikipedia.org/wiki/Software_quality\n",
      "//en.wikipedia.org/wiki/UBM_plc\n",
      "//en.wikipedia.org/wiki/Aggregate_(Data_Warehouse)\n",
      "//en.wikipedia.org/wiki/Digital_marketing\n",
      "//en.wikipedia.org/wiki/Microsoft_Academic_Search\n",
      "//en.wikipedia.org/wiki/Programming_team\n",
      "//en.wikipedia.org/wiki/Autonomous_car\n",
      "//en.wikipedia.org/wiki/MOLAP\n",
      "//en.wikipedia.org/wiki/Automatic_reasoning\n",
      "//en.wikipedia.org/wiki/Google_APIs\n",
      "//en.wikipedia.org/wiki/Conditional_independence\n",
      "//en.wikipedia.org/wiki/Google_Book_Search_Settlement_Agreement\n",
      "//en.wikipedia.org/wiki/Classification_and_regression_tree\n",
      "//en.wikipedia.org/wiki/Very-large-scale_integration\n",
      "//en.wikipedia.org/wiki/SAS_Institute\n",
      "//en.wikipedia.org/wiki/Restricted_Boltzmann_machine\n",
      "//en.wikipedia.org/wiki/STATISTICA\n",
      "//en.wikipedia.org/wiki/Minimum_message_length\n",
      "//en.wikipedia.org/wiki/Random_variables\n",
      "//en.wikipedia.org/wiki/International_Conference_on_Machine_Learning\n",
      "//en.wikipedia.org/wiki/Portal:Machine_learning\n",
      "//en.wikipedia.org/wiki/Christopher_M._Bishop\n",
      "//en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence\n",
      "//en.wikipedia.org/wiki/Spreadsheets\n",
      "//en.wikipedia.org/wiki/Crossover_(genetic_algorithm)\n",
      "//en.wikipedia.org/wiki/ID3_algorithm\n",
      "//en.wikipedia.org/wiki/Information_system\n",
      "//en.wikipedia.org/wiki/Oracle_Data_Mining\n",
      "//en.wikipedia.org/wiki/David_Rumelhart\n",
      "//en.wikipedia.org/wiki/Data_management\n",
      "//en.wikipedia.org/wiki/TD-Gammon\n",
      "//en.wikipedia.org/wiki/OLAP_cube\n",
      "//en.wikipedia.org/wiki/Data_cleaning\n",
      "//en.wikipedia.org/wiki/Entailment\n",
      "//en.wikipedia.org/wiki/Microsoft\n",
      "//en.wikipedia.org/wiki/Discovery_(observation)\n",
      "//en.wikipedia.org/wiki/Template_talk:Data_warehouse\n",
      "//en.wikipedia.org/wiki/Multithreading_(computer_architecture)\n",
      "//en.wikipedia.org/wiki/GOFAI\n",
      "//en.wikipedia.org/wiki/Data_mining\n",
      "//en.wikipedia.org/wiki/Naive_Bayes\n",
      "//en.wikipedia.org/wiki/Geoff_Hinton\n",
      "//en.wikipedia.org/wiki/Hopfield_network\n",
      "//en.wikipedia.org/wiki/Weka_(machine_learning)\n",
      "//en.wikipedia.org/wiki/Graphics_processing_unit\n",
      "//en.wikipedia.org/wiki/World_Wide_Web\n",
      "//en.wikipedia.org/wiki/Formal_language\n",
      "//en.wikipedia.org/wiki/Wikipedia:Tertiary_sources\n",
      "//en.wikipedia.org/wiki/Time_complexity\n",
      "//en.wikipedia.org/wiki/Data_warehouse_automation\n",
      "//en.wikipedia.org/wiki/Sparse_coding\n",
      "//en.wikipedia.org/wiki/Existential_risk_from_advanced_artificial_intelligence\n",
      "//en.wikipedia.org/wiki/Enterprise_software\n",
      "//en.wikipedia.org/wiki/Data_analysis\n",
      "//en.wikipedia.org/wiki/Algorithm_design\n",
      "//en.wikipedia.org/wiki/MultiDimensional_eXpressions\n",
      "//en.wikipedia.org/wiki/TensorFlow\n",
      "//en.wikipedia.org/wiki/File:Spurious_correlations_-_spelling_bee_spiders.svg\n",
      "//en.wikipedia.org/wiki/Surrogate_key\n",
      "//en.wikipedia.org/wiki/File:Kernel_Machine.svg\n",
      "//en.wikipedia.org/wiki/Genetic_algorithms\n",
      "//en.wikipedia.org/wiki/Tom_M._Mitchell\n",
      "//en.wikipedia.org/wiki/Embedded_system\n",
      "//en.wikipedia.org/wiki/Gartner\n",
      "//en.wikipedia.org/wiki/Ordinal_classification\n",
      "//en.wikipedia.org/wiki/Optical_character_recognition\n",
      "//en.wikipedia.org/wiki/Health_informatics\n",
      "//en.wikipedia.org/wiki/Category:Applied_data_mining\n",
      "//en.wikipedia.org/wiki/Network_scheduler\n",
      "//en.wikipedia.org/wiki/Sixth_normal_form\n",
      "//en.wikipedia.org/wiki/Anchor_Modeling\n",
      "//en.wikipedia.org/wiki/Evolutionary_algorithm\n",
      "//en.wikipedia.org/wiki/Networking_hardware\n",
      "//en.wikipedia.org/wiki/Data_Mining_Extensions\n",
      "//en.wikipedia.org/wiki/Template:Machine_learning_bar\n",
      "//en.wikipedia.org/wiki/Semantics_(computer_science)\n",
      "//en.wikipedia.org/wiki/Brain-machine_interfaces\n",
      "//en.wikipedia.org/wiki/HOLAP\n",
      "//en.wikipedia.org/wiki/DATADVANCE\n",
      "//en.wikipedia.org/wiki/Apriori_algorithm\n",
      "//en.wikipedia.org/wiki/Pattern_recognition\n",
      "//en.wikipedia.org/wiki/Microsoft_Analysis_Services\n",
      "//en.wikipedia.org/wiki/Bootstrap_aggregating\n",
      "//en.wikipedia.org/wiki/Trevor_Hastie\n",
      "//en.wikipedia.org/wiki/File:Wiki_letter_w.svg\n",
      "//en.wikipedia.org/wiki/Formal_methods\n",
      "//en.wikipedia.org/wiki/Occam_learning\n",
      "//en.wikipedia.org/wiki/Structured_data_analysis_(statistics)\n",
      "//en.wikipedia.org/wiki/Computational_intelligence\n",
      "//en.wikipedia.org/wiki/Hierarchical_clustering\n",
      "//en.wikipedia.org/wiki/Computer_hardware\n",
      "//en.wikipedia.org/wiki/IEEE\n",
      "//en.wikipedia.org/wiki/Internet_fraud\n",
      "//en.wikipedia.org/wiki/Statistical_model\n",
      "//en.wikipedia.org/wiki/General_Architecture_for_Text_Engineering\n",
      "//en.wikipedia.org/wiki/Cognitive_science\n",
      "//en.wikipedia.org/wiki/PSeven\n",
      "//en.wikipedia.org/wiki/Apache_Mahout\n",
      "//en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems\n",
      "//en.wikipedia.org/wiki/File:Portal-puzzle.svg\n",
      "//en.wikipedia.org/wiki/Data_dredging\n",
      "//en.wikipedia.org/wiki/Linear_discriminant_analysis\n",
      "//en.wikipedia.org/wiki/Misnomer\n",
      "//en.wikipedia.org/wiki/OpenCV\n",
      "//en.wikipedia.org/wiki/Oracle_Corporation\n",
      "//en.wikipedia.org/wiki/Data_collection\n",
      "//en.wikipedia.org/wiki/Artificial_intelligence\n",
      "//en.wikipedia.org/wiki/Conceptual_clustering\n",
      "//en.wikipedia.org/wiki/Spam_filter\n",
      "//en.wikipedia.org/wiki/Mathematical_software\n",
      "//en.wikipedia.org/wiki/Databricks\n",
      "//en.wikipedia.org/wiki/Multilinear_subspace_learning\n",
      "//en.wikipedia.org/wiki/Decision_support_system\n",
      "//en.wikipedia.org/wiki/Density_estimation\n",
      "//en.wikipedia.org/wiki/Ian_H._Witten\n",
      "//en.wikipedia.org/wiki/Video_game\n",
      "//en.wikipedia.org/wiki/List_of_artificial_intelligence_projects\n",
      "//en.wikipedia.org/wiki/Deep_belief_network\n",
      "//en.wikipedia.org/wiki/Hewlett-Packard\n",
      "//en.wikipedia.org/wiki/Statistical\n",
      "//en.wikipedia.org/wiki/Data_visualization\n",
      "//en.wikipedia.org/wiki/Multimedia_database\n",
      "//en.wikipedia.org/wiki/Operational_definition\n",
      "//en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act\n",
      "//en.wikipedia.org/wiki/DNA_sequence\n",
      "//en.wikipedia.org/wiki/Virtual_reality\n",
      "//en.wikipedia.org/wiki/Spiking_neural_network\n",
      "//en.wikipedia.org/wiki/K-means_clustering\n",
      "//en.wikipedia.org/wiki/Hierarchical_temporal_memory\n",
      "//en.wikipedia.org/wiki/Megaputer_Intelligence\n",
      "//en.wikipedia.org/wiki/Modeling_language\n",
      "//en.wikipedia.org/wiki/Stuart_J._Russell\n",
      "//en.wikipedia.org/wiki/Tensor\n",
      "//en.wikipedia.org/wiki/Integrated_Authority_File\n",
      "//en.wikipedia.org/wiki/OCLC\n",
      "//en.wikipedia.org/wiki/Sentiment_analysis\n",
      "//en.wikipedia.org/wiki/Data_science\n",
      "//en.wikipedia.org/wiki/International_Standard_Book_Number\n",
      "//en.wikipedia.org/wiki/Network_service\n",
      "//en.wikipedia.org/wiki/Loss_function\n",
      "//en.wikipedia.org/wiki/Mehryar_Mohri\n",
      "//en.wikipedia.org/wiki/Communications_of_the_ACM\n",
      "//en.wikipedia.org/wiki/StatSoft\n",
      "//en.wikipedia.org/wiki/Morgan_Kaufmann\n",
      "//en.wikipedia.org/wiki/US_Congress\n",
      "//en.wikipedia.org/wiki/Database_system\n",
      "//en.wikipedia.org/wiki/Column-oriented_DBMS\n",
      "//en.wikipedia.org/wiki/Computer_security\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "links = list(set(links))\n",
    "\n",
    "good_links = []\n",
    "for link in links:\n",
    "    if not(re.search('#|Special|php', link) or not re.search('wikipedia', link)):\n",
    "        good_links.append(link)\n",
    "        print str(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research -- was too short: \n",
      "\n",
      " The Journal of Machine Learning Research (usually abbreviated JMLR), is a scientific journal focusing on machine learning, a subfield of artificial intelligence. It was founded in 2000. The journal was founded as an open-access alternative to the journal Machine Learning. In 2001, forty editors of Machine Learning resigned in order to support JMLR, saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.[1] Print editions of JMLR were published by MIT Press until 2004, and by Microtome Publishing thereafter. Since Summer 2007 JMLR is also publishing Machine Learning Open Source Software . \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Information_integration -- was too short: \n",
      "\n",
      " Information integration (II) (also called deduplication and referential integrity) is the merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in data mining and consolidation of data from unstructured or semi-structured resources. Typically, information integration refers to textual representations of knowledge but is sometimes applied to rich-media content. Information fusion, which is a related term, involves the combination of information into a new set of information towards reducing uncertainty.[1][2] An example of technologies available to integrate information include string metrics which allow the detection of similar text in different data sources by fuzzy matching. A host of methods for these research areas are available such as those presented in the International Society of Information Fusion.  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/List_of_machine_learning_concepts -- was too short: \n",
      "\n",
      "  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Statistical_noise -- was too short: \n",
      "\n",
      " Statistical noise is the colloquialism for recognized amounts of unexplained variation in a sample.[1][2] See errors and residuals in statistics.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Rexer%27s_Annual_Data_Miner_Survey -- was too short: \n",
      "\n",
      " Rexer Analyticss Annual Data Miner Survey is the largest survey of data mining, data science, and analytics professionals in the industry. It consists of approximately 50 multiple choice and open-ended questions that cover seven general areas of data mining science and practice: (1) Field and goals, (2) Algorithms, (3) Models, (4) Tools (software packages used), (5) Technology, (6) Challenges, and (7) Future. It is conducted as a service (without corporate sponsorship) to the data mining community, and the results are usually announced at the PAW (Predictive Analytics World) conferences and shared via freely available summary reports. In the most recent survey (2013), 1259 data miners from 75 countries participated.[1] After 2011, Rexer Analytics moved to a biannual schedule.   While the five Data Miner surveys have covered many data mining topics, the three topics that get the most attention in citations and at conference presentations are:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Pedro_Domingos -- was too short: \n",
      "\n",
      " Pedro Domingos is Professor at University of Washington. He is a leading researcher in machine learning and known for markov logic network enabling uncertain inference.   Domingos received an undergraduate degree and M.S. from Technical University of Lisbon. And then at University of California, Irvine, he received an M.S. and Ph.D. After spending two years as an assistant professor at IST, he joined University of Washington in 1999 and now is a professor.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Information_Fuzzy_Networks -- was too short: \n",
      "\n",
      " Info Fuzzy Networks (IFN) is a greedy machine learning algorithm for supervised learning. The data structure produced by the learning algorithm is also called Info Fuzzy Network. IFN construction is quite similar to decision trees' construction. However, IFN constructs a directed graph and not a tree. IFN also uses the conditional mutual information metric in order to choose features during the construction stage while decision trees usually use other metrics like entropy or gini.   Input: a list of input variables that can be used, a list of data records (training set) and a minimal statistical significance used to decide whether to split a node or not (default 0.1%).\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Natural_Language_Toolkit -- was too short: \n",
      "\n",
      " The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language. NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit,[5] plus a cookbook.[6] NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.[7] NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems.   \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Agent_mining -- was too short: \n",
      "\n",
      " Agent mining is an interdisciplinary area that synergizes multiagent systems with data mining and machine learning.[1][2] The interaction and integration between multiagent systems and data mining have a long history.[3][4] The very early work on agent mining focused on agent-based knowledge discovery,[5] agent-based distributed data mining,[6][7] and agent-based distributed machine learning,[8] and using data mining to enhance agent intelligence.[9] The International Workshop on Agents and Data Mining Interaction [10] has been held for more than 10 times, co-located with the International Conference on Autonomous Agents and Multi-Agent Systems. Several proceedings are available from Springer Lecture Notes in Computer Science.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Yooreeka -- was too short: \n",
      "\n",
      " Yooreeka is a library for data mining, machine learning, soft computing, and mathematical analysis. The project started with the code of the book \"Algorithms of the Intelligent Web\".[1] Although the term \"Web\" prevailed in the title, in essence, the algorithms are valuable in any software application. It covers all major algorithms and provides many examples. Yooreeka 2.x is licensed under the Apache License rather than the somewhat more restrictive LGPL (which was the license of v1.x). The library is written 100% in the Java language. The following algorithms are covered:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Template:Data_warehouse -- was too short: \n",
      "\n",
      " How to manage this template's initial visibility\n",
      "To manage this template's visibility when it first appears, add the parameter: Unless set otherwise (see the |state= parameter in the template's code), the template's default state is autocollapse.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Early-arriving_fact -- was too short: \n",
      "\n",
      " In the Data Warehouse practice of ETL, an early fact or early-arriving fact [1] denotes the detection of a dimensional natural key during fact table source loading, prior to the assignment of a corresponding primary key or surrogate key in the dimension table. Hence, the fact which cites the dimension arrives early, relative to the definition of the dimension value. Procedurally, an early fact can be treated as:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Machine_Learning_(journal) -- was too short: \n",
      "\n",
      " Machine Learning is a peer-reviewed scientific journal, published since 1986. In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.[1] \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Software_suite -- was too short: \n",
      "\n",
      " A software suite or application suite is a collection of computer programs usually application software or programming software of related functionality, often sharing a more-or-less common user interface and some ability to smoothly exchange data with each other. Advantages Disadvantages \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/The_American_Statistician -- was too short: \n",
      "\n",
      " The American Statistician is a quarterly peer-reviewed scientific journal covering statistics published by Taylor & Francis on behalf of the American Statistical Association. It was established in 1947 and the editor-in-chief is Nicole Lazar (University of Georgia). \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/CIKM_Conference -- was too short: \n",
      "\n",
      " The ACM Conference on Information and Knowledge Management (CIKM, pronounced /sikm/) is an annual computer science research conference dedicated to information and knowledge management. Since the first event in 1992, the conference has evolved into one of the major forums for research on database management, information retrieval, and knowledge management.[1][2] The conference is noted for its interdisciplinarity, as it brings together communities that otherwise often publish at separate venues. Recent editions have attracted well beyond 500 participants.[3] In addition to the main research program, the conference also features a number of workshops, tutorials, and industry presentations.[4] For many years, the conference was held in the USA. Since 2005, venues in other countries have been selected as well. Locations include:[5]\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/MLPACK_(C%2B%2B_library) -- was too short: \n",
      "\n",
      " MLPACK is a machine learning software library for C++, built on top of the Armadillo library. MLPACK has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users.[1] Its intended target users are scientists and engineers. It is open-source software distributed under the BSD license, making it useful for developing both open source and proprietary software. Releases 1.0.11 and before were released under the LGPL license. The project is supported by the Georgia Institute of Technology and contributions from around the world.   Currently MLPACK supports the following algorithms:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/ACM_Computing_Classification_System -- was too short: \n",
      "\n",
      " The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organise subjects by area.   The system has gone through seven revisions, the first version being published in 1964, and revised versions appearing in 1982, 1983, 1987, 1991, 1998, and the now current version in 2012. The ACM Computing Classification System, version 2012, has a revolutionary change in some areas, for example, in \"Software\" that now is called \"Software and its engineering\" which has three main subjects: It is hierarchically structured in four levels. Thus, for example, one branch of the hierarchy contains: \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/MOA_(Massive_Online_Analysis) -- was too short: \n",
      "\n",
      " MOA (Massive Online Analysis)[1] is a free open-source software specific for Data stream mining with Concept drift. It is written in Java and developed at the University of Waikato, New Zealand.   MOA is an open-source framework software that allows to build and run experiments of machine learning or data mining on evolving data streams. It includes a set of learners and stream generators that can be used from the Graphical User Interface (GUI), the command-line, and the Java API. MOA contains several collections of machine learning algorithms: These algorithms are designed for large scale machine learning, dealing with concept drift, and big data streams in real time. MOA supports bi-directional interaction with Weka (machine learning). MOA is free software released under the GNU GPL.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/The_Master_Algorithm -- was too short: \n",
      "\n",
      " The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote it in order to generate interest from people outside the field. Towards the end of the book, while reviewing his invention of the Markov logic network [1] he pictures a \"master algorithm\" allowing technology to allow machine learning algorithms to asymptotically grow to a perfect understanding of how the world and people in it work.[2] \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/File:Svm_max_sep_hyperplane_with_margin.png -- was too short: \n",
      "\n",
      " Other reasons this message may be displayed:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/NLTK -- was too short: \n",
      "\n",
      " The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language. NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit,[5] plus a cookbook.[6] NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.[7] NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems.   \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Mallet_(software_project) -- was too short: \n",
      "\n",
      " MALLET is a Java \"MAchine Learning for LanguagE Toolkit\".   MALLET is an integrated collection of Java code useful for statistical natural language processing, document classification, cluster analysis, information extraction, topic modeling and other machine learning applications to text. MALLET was developed primarily by Andrew McCallum, of the University of Massachusetts Amherst, with assistance from graduate students and faculty from both UMASS and the University of Pennsylvania. \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Glossary_of_artificial_intelligence -- was too short: \n",
      "\n",
      " This glossary of artificial intelligence terms is about artificial intelligence, its sub-disciplines, and related fields.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Leo_Breiman -- was too short: \n",
      "\n",
      " Leo Breiman (January 27, 1928  July 5, 2005) was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards, and was a member of the United States National Academy of Science. Breiman's work helped to bridge the gap between statistics and computer science, particularly in the field of machine learning. His most important contributions were his work on classification and regression trees and ensembles of trees fit to bootstrap samples. Bootstrap aggregation was given the name bagging by Breiman. Another of Breiman's ensemble approaches is the random forest. \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Computational_mathematics -- was too short: \n",
      "\n",
      " Computational mathematics involves mathematical research in areas of science where computing plays a central and essential role, emphasizing algorithms, numerical methods, and symbolic methods. Computation in the research is prominent.[1] Computational mathematics emerged as a distinct part of applied mathematics by the early 1950s. Currently, computational mathematics can refer to or include:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Category:Data_mining_and_machine_learning_software -- was too short: \n",
      "\n",
      " Some products in Category:Data analysis software and Category:Statistical software also include data mining and machine learning facilities. This category has only the following subcategory. The following 77 pages are in this category, out of 77 total. This list may not reflect recent changes (learn more).\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Java_Data_Mining -- was too short: \n",
      "\n",
      " Java Data Mining (JDM) was a standard Java API for developing data mining applications and tools. JDM defines an object model and Java API for data mining objects and processes. JDM enables applications to integrate data mining technology for developing predictive analytics applications and tools. The JDM 1.0 standard was developed under the Java Community Process as JSR 73.[1] In 2006, the JDM 2.0 specification was being developed under JSR 247, but has been withdrawn in 2011 without standardization.[2] Various data mining functions and techniques like statistical classification and association, regression analysis, data clustering, and attribute importance are covered by the 1.0 release of this standard. \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Business_intelligence_tools -- was too short: \n",
      "\n",
      " Business intelligence tools are a type of application software designed to retrieve, analyze, transform and report data for business intelligence. The tools generally read data that have been previously stored, often, though not necessarily, in a data warehouse or data mart.   The key general categories of business intelligence tools are: Except for spreadsheets, these tools are provided as standalone tools, suites of tools, components of ERP systems, or as components of software targeted to a specific industry. The tools are sometimes packaged into data warehouse appliances.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Encog -- was too short: \n",
      "\n",
      " Encog is a machine learning framework available for Java, .Net, and C++. [1] Encog supports different learning algorithms such as Bayesian Networks, Hidden Markov Models and Support Vector Machines. However, its main strength lies in its neural network algorithms. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using many different techniques. Multithreading is used to allow optimal training performance on multicore machines. The C++ version of Encog can offload some processing to an OpenCL compatible GPU for further performance gains. Encog can be used for many tasks, including medical[2] and financial research.[3] A GUI based workbench is also provided to help model and train neural networks. Encog has been in active development since 2008.[4]  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/File:Animation2.gif -- was too short: \n",
      "\n",
      " Other reasons this message may be displayed:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Comparison_of_OLAP_Servers -- was too short: \n",
      "\n",
      " The following tables compare general and technical information for a number of online analytical processing (OLAP) servers. Please see the individual products articles for further information.   APIs and query languages OLAP servers support. A list of OLAP features that are not supported by all vendors. All vendors support features such as parent-child, multilevel hierarchy, drilldown. The OLAP servers can run on the following operating systems: Note (1):The server availability depends on Java Virtual Machine not on the operating system\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Scikit-image -- was too short: \n",
      "\n",
      " scikit-image (formerly scikits.image) is an open source image processing library for the Python programming language.[2] It includes algorithms for segmentation, geometric transformations, color space manipulation, analysis, filtering, morphology, feature detection, and more.[3] It is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.   The scikit-image project started as scikits.image, by Stfan van der Walt. Its name stems from the notion that it is a \"SciKit\" (SciPy Toolkit), a separately-developed and distributed third-party extension to SciPy.[4] The original codebase was later extensively rewritten by other developers. Of the various scikits, scikit-image as well as scikit-learn were described as \"well-maintained and popular\" in November 2012[update].[5] Scikit-image has also been active in the Google Summer of Code.[6] scikit-image is largely written in Python, with some core algorithms written in Cython to achieve performance.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Intention_mining -- was too short: \n",
      "\n",
      " In data mining, intention mining or intent mining is the problem of determining a user's intention from logs of their behavior in interaction with a computer system, such as a search engine. This notion is introduced for the first time in the paper of Dr. Ghazaleh Khodabandelou et al.[1] Some authors model the intentions as an intentional process model in order to have a better understanding of the human way of thinking.[1] Intention Mining has already been used in several domains:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Category:Data_warehousing_products -- was too short: \n",
      "\n",
      " This category lists products, mostly software, that are used in data warehousing. It can also contain specialised producers of these products, including those producing open source products. This category has only the following subcategory. The following 36 pages are in this category, out of 36 total. This list may not reflect recent changes (learn more).\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Template:Computer_science -- was too short: \n",
      "\n",
      " How to manage this template's initial visibility\n",
      "To manage this template's visibility when it first appears, add the parameter: Unless set otherwise (see the |state= parameter in the template's code), the template's default state is autocollapse.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Aggregate_function -- was too short: \n",
      "\n",
      " In database management an aggregate function is a function where the values of multiple rows are grouped together as input on certain criteria to form a single value of more significant meaning or measurement such as a set, a bag or a list. Common aggregate functions include: Aggregate functions are common in numerous programming languages, spreadsheets, and relational algebra. https://msdn.microsoft.com/en-IN/library/ms173454.aspx \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/List_of_reporting_software -- was too short: \n",
      "\n",
      " The following is a list of notable report generator software. Reporting software is used to generate human-readable reports from various data sources.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Data_Mining_and_Knowledge_Discovery -- was too short: \n",
      "\n",
      " Data Mining and Knowledge Discovery is a triannual peer-reviewed scientific journal focusing on data mining. It is published by Springer Science+Business Media. As of 2012[update], the editor-in-chief is Geoffrey I. Webb. It was started in 1996 and launched in 1997 by Usama Fayyad as founding Editor-in-Chief by Kluwer Academic Publishers (later becoming Springer). The first Editorial provides a summary of why it was started.[1] Since its founding in 1997, this Journal has become the most influential academic journal in the field. It has one of the highest index ratings and is considered authoritative academically. \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Chemicalize.org -- was too short: \n",
      "\n",
      " chemicalize.org is a free chemical structure miner and web search engine developed and owned by ChemAxon. The main purpose of chemicalize.org is to identify chemical names (SMILES, traditional and IUPAC names) on websites and convert them to chemical structures. chemicalize.org provides other services such as structure based predictions, chemical search, and a chemicalized web search.   2009 Chemicalize a webpage May 2010 Calculate properties November 2010 Chemical search March 2011 Web search\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Conference_on_Information_and_Knowledge_Management -- was too short: \n",
      "\n",
      " The ACM Conference on Information and Knowledge Management (CIKM, pronounced /sikm/) is an annual computer science research conference dedicated to information and knowledge management. Since the first event in 1992, the conference has evolved into one of the major forums for research on database management, information retrieval, and knowledge management.[1][2] The conference is noted for its interdisciplinarity, as it brings together communities that otherwise often publish at separate venues. Recent editions have attracted well beyond 500 participants.[3] In addition to the main research program, the conference also features a number of workshops, tutorials, and industry presentations.[4] For many years, the conference was held in the USA. Since 2005, venues in other countries have been selected as well. Locations include:[5]\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Angoss -- was too short: \n",
      "\n",
      " Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, is a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC. In June 2013 the private equity firm Peterson Partners acquired Angoss for $8.4 million.[1]  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/File:Internet_map_1024.jpg -- was too short: \n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/ML-Flex -- was too short: \n",
      "\n",
      " ML-Flex is a free and open-source software package used for machine-learning (or data mining) analyses. In particular, it provides support for classification problems. ML-Flex enables users to:[1]\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Academic_Press -- was too short: \n",
      "\n",
      " Academic Press is an academic book publisher. Originally independent, it was acquired by Harcourt, Brace & World in 1969. Reed Elsevier bought Harcourt in 2000, and Academic Press is now an imprint of Elsevier. Academic Press publishes reference books, serials and online products in the subject areas of: Well-known products include the Methods in Enzymology series and encyclopedias such as The International Encyclopedia of Public Health and the Encyclopedia of Neuroscience. \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach -- was too short: \n",
      "\n",
      " Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. The third edition of the book was released 11 December 2009. It is used in over 1100 universities worldwide[1] and has been called \"the most popular artificial intelligence textbook in the world\".[2] The book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.   Artificial Intelligence: A Modern Approach is divided into seven parts with a total of 27 chapters.[3] The authors state that it is a large text which would take two semesters to cover all the chapters and projects. Programs in the book are presented in pseudo code with implementations in Java, Python, and Lisp available online.[4] There are also unsupported implementations in Prolog, C++, and C#. \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Template_talk:Computer_science -- was too short: \n",
      "\n",
      " This part is a bit short. But does OLAP really belong into here? is it really \"major\"? I've replaced it with various major database topics such as database indexing, transaction processing and DBMS in general. These links are more important than OLAP. --Chire (talk) 00:05, 29 June 2010 (UTC)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Measure_(data_warehouse) -- was too short: \n",
      "\n",
      " In a data warehouse, a measure is a property on which calculations (e.g., sum, count, average, minimum, maximum) can be made.   For example if retail store sold a specific product, the quantity and prices of each item sold could be added or averaged to find the total number of items sold and total or average price of the goods sold. When entering data into a metadata registry such as ISO/IEC 11179, representation terms such as Number, Value and Measure are typically used as measures.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/International_Conference_on_Very_Large_Data_Bases -- was too short: \n",
      "\n",
      " VLDB is an annual conference held by the non-profit Very Large Data Base Endowment Inc. The mission of VLDB is to promote and exchange scholarly work in databases and related fields throughout the world. The VLDB conference began in 1975 and is now closely associated with SIGMOD and SIGKDD.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/List_of_machine_learning_algorithms -- was too short: \n",
      "\n",
      "  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/International_Conference_on_Machine_Learning -- was too short: \n",
      "\n",
      " The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning, attracting annually more than 2000 participants from all over the world. It is supported by the International Machine Learning Society (IMLS). The conference attracts leading innovations in the field of machine learning. ICML is a top tier conference, and is one of the two most impactful conferences in Machine Learning (along with Conference on Neural Information Processing Systems).   The first ICML was held 1980 in Pittsburgh.[1] The conference completed its 25th anniversary in Helsinki, Finland in 2008.[2] Recent conferences were held in Haifa, Israel (2010), Bellevue, Washington, USA (2011), Edinburgh, UK (2012), Atlanta, Georgia, USA (2013), and Beijing, China (2014), Lille, France (2015), New York City, USA (2016). \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Christopher_M._Bishop -- was too short: \n",
      "\n",
      " Christopher Michael Bishop (born 7 April 1959) FREng, FRSE, is the Laboratory Director at Microsoft Research Cambridge and professor of Computer Science at the University of Edinburgh and a Fellow of Darwin College, Cambridge.[6] Chris obtained a Bachelor of Arts degree in Physics from St Catherine's College, Oxford, and a PhD in Theoretical Physics from the University of Edinburgh, with a thesis on quantum field theory. [1][6]\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Template_talk:Data_warehouse -- was too short: \n",
      "\n",
      " The template misses information about products in data warehousing (and their companies). Only some companies that produce OLAP-servers are mentioned. Possible options: -DePiep (talk) 14:51, 22 September 2010 (UTC)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/File:Spurious_correlations_-_spelling_bee_spiders.svg -- was too short: \n",
      "\n",
      " Other reasons this message may be displayed:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/File:Kernel_Machine.svg -- was too short: \n",
      "\n",
      " Other reasons this message may be displayed:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Category:Applied_data_mining -- was too short: \n",
      "\n",
      " Notable applications and use of data mining The following 23 pages are in this category, out of 23 total. This list may not reflect recent changes (learn more).\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Template:Machine_learning_bar -- was too short: \n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/DATADVANCE -- was too short: \n",
      "\n",
      " DATADVANCE Is a joint venture company between AIRBUS Group and private Groups Spin-off Academics Russia, evolved out of a collaborative research program between AIRBUS Group and Institute for Information Transmission Problems [1] of the Russian Academy of Sciences (IITP RAS). MACROS algorithmic core, embedded in pSeven,[2] provides unique proprietary and state-of-the-art algorithms for dimension reduction, design of experiments, sensitivity analysis, meta-modeling, uncertainty quantification as well as modern single, multi-objective and robust optimization strategies.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/File:Wiki_letter_w.svg -- was too short: \n",
      "\n",
      " GFDL GNU Free Documentation License //en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License This licensing tag was added to this file as part of the GFDL licensing update. Cc-by-sa-3.0 Attribution-ShareAlike 3.0 http://creativecommons.org/licenses/by-sa/3.0/\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Structured_data_analysis_(statistics) -- was too short: \n",
      "\n",
      " Structured data analysis is the statistical data analysis of structured data. This can arise either in the form of an a priori structure such as multiple-choice questionnaires or in situations with the need to search for structure that fits the given data, either exactly or approximately. This structure can then be used for making comparisons, predictions, manipulations etc.[1][2] \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/File:Portal-puzzle.svg -- was too short: \n",
      "\n",
      " In case this is not legally possible:\n",
      "The copyright holder grants any entity the right to use this work for any purpose, without any conditions, unless such conditions are required by law. Public domain //en.wikipedia.org/wiki/File:Portal-puzzle.svg\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Misnomer -- was too short: \n",
      "\n",
      " A misnomer is a word or term that suggests a meaning that is known to be wrong. Misnomers often arise because the thing named received its name long before its true nature was known. A misnomer may also be simply a word that is used incorrectly or misleadingly.[1] \"Misnomer\" does not mean \"misunderstanding\" or \"popular misconception\".[1]   Some of the sources of misnomers are:\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Ian_H._Witten -- was too short: \n",
      "\n",
      " Ian H. Witten is a computer scientist at the University of Waikato, New Zealand. He received his Ph.D. in 1976 from the University of Essex, England (Electrical Engineering Science).[1] Witten is a co-creator of the Sequitur algorithm[2] and original creator of the WEKA software package for data mining.   \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/List_of_artificial_intelligence_projects -- was too short: \n",
      "\n",
      " The following is a list of current and past, nonclassified notable artificial intelligence projects.  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Communications_of_the_ACM -- was too short: \n",
      "\n",
      " Communications of the ACM is the monthly magazine of the Association for Computing Machinery (ACM). It was established in 1957, with Saul Rosen its first managing editor. It is sent to all ACM members.[1][2] Articles are intended for readers with backgrounds in all areas of computer science and information systems. The focus is on the practical implications of advances in information technology and associated management issues; ACM also publishes a variety of more theoretical journals. The magazine straddles the boundary of a science magazine, trade magazine, and a scientific journal. While the content is subject to peer review, the articles published are often summaries of research that may also be published elsewhere. Material published must be accessible and relevant to a broad readership.[3] From 1960 onward, CACM also published algorithms, expressed in ALGOL. The collection of algorithms later became known as the Collected Algorithms of the ACM.[4]\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_set = {}\n",
    "\n",
    "# Scrape each article\n",
    "for article in good_links:\n",
    "    myScraper = WikipediaFetcher()\n",
    "    scraped_article = myScraper.fetch(page_name=article)\n",
    "    \n",
    "    # If succesfully scraped\n",
    "    if scraped_article['success'] == True:\n",
    "        \n",
    "        # Use beautiful soup to parse HTML\n",
    "        soup = BeautifulSoup(scraped_article['content'], 'html.parser')\n",
    "        \n",
    "        # FInd all p tags which seem to contain the body of the articles\n",
    "        get_tags = soup.find_all(['p'])        \n",
    "        \n",
    "        document_text = \"\"        \n",
    "        # For each tag found, strip extra characters and append to a document text string\n",
    "        for tag in get_tags:    \n",
    "            document_text = document_text + \" \" + (''.join(tag.findAll(text=True))).strip()                \n",
    "        \n",
    "        # If the document is of sufficient size\n",
    "        if len(document_text) > 1000:                \n",
    "            document_set[article]  = document_text\n",
    "        else:\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print article + \" -- was too short: \"\n",
    "            print \"\"\n",
    "            print document_text\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print \"\"\n",
    "            \n",
    "    else:\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print article + \" -- failed to scrape\"\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from random import shuffle\n",
    "\n",
    "dataset = document_set.values()\n",
    "\n",
    "# Check for and remove duplicates \n",
    "dupes = [item for item, count in collections.Counter(dataset).items() if count > 1]\n",
    "\n",
    "# Shuffle order for model training\n",
    "if len(dupes) > 0:\n",
    "    print \"Duplicates found.\"\n",
    "    dataset = list(set(dataset))\n",
    "    shuffle(dataset)\n",
    "else:\n",
    "    shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Tokenizer For Clustering ###\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Porter stemmer is a pre-trained stemmer that finds the most likely stem for an english word\n",
    "from gensim.parsing import PorterStemmer\n",
    "global_stemmer = PorterStemmer()\n",
    "\n",
    "# Make a helper that will return the original form of the stem in future methods\n",
    "class StemmingHelper(object):\n",
    "    \"\"\"\n",
    "    Class to aid the stemming process - from word to stemmed form,\n",
    "    and vice versa.\n",
    "    The 'original' form of a stemmed word will be returned as the\n",
    "    form in which its been used the most number of times in the text.\n",
    "    \"\"\"\n",
    " \n",
    "    #This reverse lookup will remember the original forms of the stemmed words\n",
    "    word_lookup = {}\n",
    " \n",
    "    @classmethod\n",
    "    def stem(cls, word):\n",
    "        \"\"\"\n",
    "        Stems a word and updates the reverse lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        #Stem the word\n",
    "        stemmed = global_stemmer.stem(word)\n",
    " \n",
    "        #Update the word lookup\n",
    "        if stemmed not in cls.word_lookup:\n",
    "            cls.word_lookup[stemmed] = {}\n",
    "        cls.word_lookup[stemmed][word] = (\n",
    "            cls.word_lookup[stemmed].get(word, 0) + 1)\n",
    " \n",
    "        return stemmed\n",
    " \n",
    "    @classmethod\n",
    "    def original_form(cls, word):\n",
    "        \"\"\"\n",
    "        Returns original form of a word given the stemmed version,\n",
    "        as stored in the word lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        if word in cls.word_lookup:\n",
    "            return max(cls.word_lookup[word].keys(),\n",
    "                       key=lambda x: cls.word_lookup[word][x])\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "# Remove unicode, normalizing it to ascii\n",
    "import unicodedata\n",
    "\n",
    "# Custom tokenizer which produces sentence-wise tokens\n",
    "def sentence_tokenize(doc):\n",
    "        doc = unicodedata.normalize('NFKD', doc).encode('ascii','ignore')\n",
    "        doc = doc.lower()\n",
    "        doc = re.sub('[-]', ' ', doc)\n",
    "        doc = re.sub('[\\(\\[].*?[\\)\\]]', ' ', doc)\n",
    "        doc = re.sub(' +',' ', doc)\n",
    "                \n",
    "        sentences = nltk.sent_tokenize(doc) \n",
    "        \n",
    "        sentence_token_list = []\n",
    "        for sentence in sentences:\n",
    "            # Filter sentences with less than 5 words since they won't provide much context\n",
    "            if len(sentence) > 4:                \n",
    "                sentence_token_list.append([\n",
    "                        StemmingHelper.stem(word) \n",
    "                        for word in nltk.word_tokenize(sentence)\n",
    "                        # Filter words with no characters since they usually are numeric gibberish\n",
    "                        if len(re.sub('[^a-z]', '', word)) > 0\n",
    "                    ])       \n",
    "\n",
    "        return sentence_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc = document_set['//en.wikipedia.org/wiki/Natural_language_processing']\n",
    "\n",
    "# import unicodedata\n",
    "# import nltk\n",
    "\n",
    "# doc = unicodedata.normalize('NFKD', doc).encode('ascii','ignore')\n",
    "# doc = doc.lower()\n",
    "# doc = re.sub('[-]', ' ', doc)\n",
    "# doc = re.sub('[\\(\\[].*?[\\)\\]]', ' ', doc)\n",
    "# doc = re.sub(' +',' ', doc)\n",
    "\n",
    "# sentences = nltk.sent_tokenize(doc) \n",
    "\n",
    "# for sentence in sentences:\n",
    "#     print sentence\n",
    "#     print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build massive list of  sentences that are tokenized\n",
    "corpus = []\n",
    "for text in dataset:\n",
    "    corpus.extend(sentence_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimension = 250\n",
    "model = Word2Vec(corpus, size=dimension, window=4, min_count=5, sg=1, sample=0.001, workers=4, iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6829\n"
     ]
    }
   ],
   "source": [
    "vocab = list(model.vocab.keys())\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar to: vector\n",
      "hyperplane\n",
      "matrix\n",
      "nearest\n",
      "weights\n",
      "dimensional\n",
      "------------------------------------------\n",
      "Similar to: clustering\n",
      "subspace\n",
      "centroid\n",
      "linkage\n",
      "nearest\n",
      "hierarchical\n",
      "------------------------------------------\n",
      "Similar to: statistical\n",
      "mathematical\n",
      "bayesian\n",
      "proposition\n",
      "multivariate\n",
      "kolmogorov\n",
      "------------------------------------------\n",
      "Similar to: model\n",
      "predictive\n",
      "approach\n",
      "methods\n",
      "fit\n",
      "markov\n",
      "------------------------------------------\n",
      "Similar to: deep\n",
      "feedforward\n",
      "learning\n",
      "neural\n",
      "supervised\n",
      "backpropagation\n",
      "------------------------------------------\n",
      "Similar to: neural\n",
      "network\n",
      "recurrent\n",
      "feedforward\n",
      "artificial\n",
      "deep\n",
      "------------------------------------------\n",
      "Similar to: network\n",
      "neural\n",
      "connected\n",
      "layer\n",
      "routers\n",
      "internetworking\n",
      "------------------------------------------\n",
      "Similar to: training\n",
      "learning\n",
      "unseen\n",
      "input\n",
      "reward\n",
      "rnn\n",
      "------------------------------------------\n",
      "Similar to: supervised\n",
      "unsupervised\n",
      "learning\n",
      "semi\n",
      "transduction\n",
      "deep\n",
      "------------------------------------------\n",
      "Similar to: unsupervised\n",
      "supervised\n",
      "learning\n",
      "autoencoder\n",
      "semi\n",
      "deep\n",
      "------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = \"vector cluster statistic model deep neural network train supervise unsupervise\"\n",
    "\n",
    "for stem in (StemmingHelper.stem(word) for word in test.split()):\n",
    "    print \"Similar to: \" + StemmingHelper.original_form(stem)\n",
    "    for similar in model.most_similar(stem)[0:5]:\n",
    "        print StemmingHelper.original_form(similar[0])\n",
    "    print \"------------------------------------------\"    \n",
    "print \"\"\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class SOM(object):\n",
    "    \"\"\"\n",
    "    2-D Self-Organizing Map\n",
    "    - Euclidean metric\n",
    "    - Gaussian Neighbourhood function\n",
    "    - Linearly decreasing learning rate\n",
    "    \"\"\"\n",
    " \n",
    "\n",
    "    #To check if the SOM has been trained\n",
    "    _trained = False\n",
    " \n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Initializes all components of the TensorFlow Graph.\n",
    "        - m X n are the dimensions of the SOM\n",
    "        - 'n_iterations' is an integer denoting # iterations for training\n",
    "        - 'dim' is the dimensionality of the training inputs\n",
    "        - 'alpha' is a number denoting the initial learning rate\n",
    "            >> default = 0.3\n",
    "        - 'sigma' is the the initial neighbourhood value\n",
    "            >> the radius of influence of the BMU while training\n",
    "            >> default = max(m, n) / 2.0\n",
    "        \"\"\"\n",
    " \n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    "\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "            \n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "            \n",
    "        # Normalizing constant for decay of alpha\n",
    "        time_constant = self._n_iterations/np.log(sigma)\n",
    "            \n",
    " \n",
    "        #Initialize Graph\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        #Populate graph\n",
    "        with self._graph.as_default():\n",
    "            ##Placeholders to be fed in during training \n",
    "            \n",
    "            #The training vector\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            #Iteration number\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            #Randomly initialized weights for neurons\n",
    "            #Stored as a matrix variable of shape [m*n, dim]\n",
    "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            #Grid locations of neurons\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "\n",
    "            #Compute the Best Matching Unit given a vector\n",
    "            #argmin of Euclidean distance b/w weights and given input\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.sub(self._weightage_vects, tf.pack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            #Extract locations\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]))),\n",
    "                                 [2])\n",
    " \n",
    "            #Adjust alpha and sigma values based on iteration\n",
    "            learning_rate_op = tf.exp(tf.neg(tf.div(self._iter_input,\n",
    "                                                  time_constant))) \n",
    "        \n",
    "            _alpha_op = tf.mul(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.mul(sigma, learning_rate_op)\n",
    " \n",
    "            #Generate learning rate for all neurons\n",
    "            #Based on iteration number and location wrt BMU\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.sub(\n",
    "                self._location_vects, tf.pack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            \n",
    "            neighbourhood_func = tf.exp(tf.neg(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            \n",
    "            learning_rate_op = tf.mul(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            #Update weight vectors of all neurons\n",
    "            learning_rate_multiplier = tf.pack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.mul(\n",
    "                learning_rate_multiplier,\n",
    "                tf.sub(tf.pack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects))                                         \n",
    "            \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            \n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            #Initialize session and variables\n",
    "            self._sess = tf.Session() \n",
    "            init_op = tf.initialize_all_variables()\n",
    "            self._sess.run(init_op)\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        \"\"\"\n",
    "        Yields one by one the 2-D locations of the individual neurons\n",
    "        in the SOM.\n",
    "        \"\"\"\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM\n",
    "        \"\"\"\n",
    " \n",
    "        #Training iterations\n",
    "        for iter_no in range(self._n_iterations):\n",
    "            #Train with each vector one by one\n",
    "            for input_vect in input_vects:\n",
    "                self._sess.run(self._training_op,\n",
    "                               feed_dict={self._vect_input: input_vect,\n",
    "                                          self._iter_input: iter_no})\n",
    " \n",
    "        #Store a centroid grid for easy retrieval later on\n",
    "        centroid_grid = [[] for i in range(self._m)]\n",
    "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "        self._locations = list(self._sess.run(self._location_vects))\n",
    "        for i, loc in enumerate(self._locations):\n",
    "            centroid_grid[loc[0]].append(self._weightages[i])\n",
    "        self._centroid_grid = centroid_grid\n",
    " \n",
    "        self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        \"\"\"\n",
    "        Returns a list of 'm' lists, with each inner list containing\n",
    "        the 'n' corresponding centroid locations as 1-D NumPy arrays.\n",
    "        \"\"\"\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        \"\"\"\n",
    "        Maps each input vector to neuron in the SOM grid.\n",
    "               \n",
    "        Returns a list of 1-D NumPy arrays containing (x, y) for each input.\n",
    "        \"\"\"\n",
    " \n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For plotting the images\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "myStop = stopwords.words('english')\n",
    "\n",
    "filtered_vocab = list(set(vocab)-set([global_stemmer.stem(x) for x in myStop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for sentence in corpus:\n",
    "    words.extend([x for x in sentence if x in filtered_vocab])\n",
    "\n",
    "from collections import Counter\n",
    "word_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used\n",
      "data\n",
      "computer\n",
      "system\n",
      "model\n",
      "'s\n",
      "also\n",
      "information\n",
      "learning\n",
      "algorithm\n",
      "may\n",
      "program\n",
      "time\n",
      "general\n",
      "include\n",
      "function\n",
      "example\n",
      "network\n",
      "process\n",
      "set\n",
      "development\n",
      "problem\n",
      "software\n",
      "based\n",
      "language\n",
      "methods\n",
      "different\n",
      "many\n",
      "research\n",
      "machine\n",
      "number\n",
      "operating\n",
      "two\n",
      "states\n",
      "first\n",
      "value\n",
      "applications\n",
      "provide\n",
      "new\n",
      "user\n",
      "design\n",
      "called\n",
      "variables\n",
      "distribution\n",
      "analysis\n",
      "work\n",
      "results\n",
      "required\n",
      "make\n",
      "probability\n",
      "term\n",
      "allows\n",
      "form\n",
      "would\n",
      "access\n",
      "type\n",
      "often\n",
      "technology\n",
      "statistical\n",
      "case\n",
      "means\n",
      "training\n",
      "performance\n",
      "however\n",
      "given\n",
      "specific\n",
      "related\n",
      "like\n",
      "features\n",
      "point\n",
      "following\n",
      "communication\n",
      "approach\n",
      "code\n",
      "defined\n",
      "database\n",
      "product\n",
      "united\n",
      "control\n",
      "possible\n",
      "measure\n",
      "created\n",
      "well\n",
      "large\n",
      "theory\n",
      "object\n",
      "intelligence\n",
      "market\n",
      "way\n",
      "structure\n",
      "field\n",
      "company\n",
      "business\n",
      "input\n",
      "source\n",
      "order\n",
      "known\n",
      "predictive\n",
      "human\n",
      "science\n",
      "support\n",
      "standard\n",
      "need\n",
      "test\n",
      "services\n",
      "represent\n",
      "techniques\n",
      "since\n",
      "group\n",
      "level\n",
      "engineering\n",
      "published\n",
      "internet\n",
      "implementation\n",
      "described\n",
      "class\n",
      "change\n",
      "complex\n",
      "security\n",
      "open\n",
      "management\n",
      "space\n",
      "study\n",
      "organization\n",
      "digital\n",
      "search\n",
      "web\n",
      "rules\n",
      "linear\n",
      "referred\n",
      "could\n",
      "university\n",
      "several\n",
      "world\n",
      "considered\n",
      "documents\n",
      "common\n",
      "image\n",
      "part\n",
      "important\n",
      "activities\n",
      "layer\n",
      "years\n",
      "clustering\n",
      "applied\n",
      "vector\n",
      "random\n",
      "neural\n",
      "public\n",
      "decision\n",
      "available\n",
      "produce\n",
      "non\n",
      "another\n",
      "typically\n",
      "high\n",
      "optimization\n",
      "project\n",
      "mathematical\n",
      "nsa\n",
      "dependent\n",
      "run\n",
      "library\n",
      "selection\n",
      "error\n",
      "natural\n",
      "tasks\n",
      "observed\n",
      "single\n",
      "name\n",
      "knowledge\n",
      "usually\n",
      "effect\n",
      "original\n",
      "collection\n",
      "key\n",
      "games\n",
      "tools\n",
      "components\n",
      "made\n",
      "even\n",
      "associated\n",
      "solution\n",
      "within\n",
      "take\n",
      "online\n",
      "government\n",
      "find\n",
      "particular\n",
      "must\n",
      "individual\n",
      "found\n",
      "mining\n",
      "factor\n",
      "involves\n",
      "multiple\n",
      "contain\n",
      "increase\n",
      "real\n",
      "existing\n"
     ]
    }
   ],
   "source": [
    "short_list = word_counts.most_common(200)\n",
    "for wordCount in short_list:\n",
    "    print StemmingHelper.original_form(wordCount[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "# short_list = [StemmingHelper.stem(x) for x in 'algorithm activation approximation architecture artificial assumptions bayesian class classification classifier clustering conditional connected converge convolutional data decision deep density dependent dimensional distance distribution error estimate expected features fit function gene gradient hidden independent information iterative latent layer learning likelihood linear local logistic machine map markov matrix maximum means measure memory minimize model network neural neurons nodes normal optimal output parameters pattern perceptron prediction probability random recognition regression represent representation rule space squared standard state statistical structure supervised support test training tree variables variance vector weights'\n",
    "#               .split()]\n",
    "\n",
    "short_list = list(set(short_list))\n",
    "print len(short_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_list = []\n",
    "\n",
    "for x in short_list:\n",
    "    vector_list.append(model[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = np.vstack(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training inputs\n",
    "\n",
    "grid_size = 8\n",
    "\n",
    "som = SOM(m=grid_size, n=int(1.2*grid_size), dim=dimension, n_iterations=500)\n",
    "som.train(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n",
      "overlap found\n"
     ]
    }
   ],
   "source": [
    "#Map to their closest neurons\n",
    "mapped = som.map_vects(vector_list)\n",
    "\n",
    "mapped_flat = {}\n",
    "for i, m in enumerate(mapped):\n",
    "    if not (m[0] in mapped_flat):\n",
    "        mapped_flat[m[0]] = {}\n",
    "    if not (m[1] in mapped_flat[m[0]]):\n",
    "        mapped_flat[m[0]][m[1]] = StemmingHelper.original_form(short_list[i])\n",
    "    else:\n",
    "        mapped_flat[m[0]][m[1]] = mapped_flat[m[0]][m[1]] + \"; \" + StemmingHelper.original_form(short_list[i])\n",
    "        print \"overlap found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get output grid\n",
    "grid = som.get_centroids()\n",
    "\n",
    "centroids = []\n",
    "for m_list in grid:\n",
    "    for centroid in m_list:\n",
    "        centroids.append(centroid)\n",
    "\n",
    "indexes = []\n",
    "for x in mapped_flat:\n",
    "    for y in mapped_flat[x]:\n",
    "        indexes.append(y*x)\n",
    "indexes = list(set(indexes))\n",
    "\n",
    "mapped_exist = {}\n",
    "for index in range(0, len(centroids)):\n",
    "    if index in indexes:\n",
    "        if not index in mapped_exist:\n",
    "            mapped_exist[index] = centroids[index]\n",
    "\n",
    "to_reduce = []\n",
    "for x in mapped_exist:\n",
    "    to_reduce.append(mapped_exist[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Deep Auto-Encoder implementation\n",
    "\t\n",
    "\tAn auto-encoder works as follows:\n",
    "\n",
    "\tData of dimension k is reduced to a lower dimension j using a matrix multiplication:\n",
    "\tsoftmax(W*x + b)  = x'\n",
    "\t\n",
    "\twhere W is matrix from R^k --> R^j\n",
    "\n",
    "\tA reconstruction matrix W' maps back from R^j --> R^k\n",
    "\n",
    "\tOur reconstruction function is softmax'(W' * x' + b') \n",
    "\n",
    "\tNow the point of the auto-encoder is to create a reduction matrix (values for W, b) \n",
    "\tthat is \"good\" at reconstructing  the original data. \n",
    "\n",
    "\tThus we want to minimize  ||softmax'(W' * (softmax(W *x+ b)) + b')  - x||\n",
    "\n",
    "\tA deep auto-encoder is nothing more than stacking successive layers of these reductions.\n",
    "\"\"\"\n",
    "import math\n",
    "import random\n",
    "\n",
    "def create(x, layer_sizes):\n",
    "\n",
    "\t# Build the encoding layers\n",
    "\tnext_layer_input = x\n",
    "\n",
    "\tencoding_matrices = []\n",
    "\tfor dim in layer_sizes:\n",
    "\t\tinput_dim = int(next_layer_input.get_shape()[1])\n",
    "\n",
    "\t\t# Initialize W using random values in interval [-1/sqrt(n) , 1/sqrt(n)]\n",
    "\t\tW = tf.Variable(tf.random_uniform([input_dim, dim], -1.0 / math.sqrt(input_dim), 1.0 / math.sqrt(input_dim)))\n",
    "\n",
    "\t\t# Initialize b to zero\n",
    "\t\tb = tf.Variable(tf.zeros([dim]))\n",
    "\n",
    "\t\t# We are going to use tied-weights so store the W matrix for later reference.\n",
    "\t\tencoding_matrices.append(W)\n",
    "\n",
    "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
    "\n",
    "\t\t# the input into the next layer is the output of this layer\n",
    "\t\tnext_layer_input = output\n",
    "\n",
    "\t# The fully encoded x value is now stored in the next_layer_input\n",
    "\tencoded_x = next_layer_input\n",
    "\n",
    "\t# build the reconstruction layers by reversing the reductions\n",
    "\tlayer_sizes.reverse()\n",
    "\tencoding_matrices.reverse()\n",
    "\n",
    "\n",
    "\tfor i, dim in enumerate(layer_sizes[1:] + [ int(x.get_shape()[1])]) :\n",
    "\t\t# we are using tied weights, so just lookup the encoding matrix for this step and transpose it\n",
    "\t\tW = tf.transpose(encoding_matrices[i])\n",
    "\t\tb = tf.Variable(tf.zeros([dim]))\n",
    "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
    "\t\tnext_layer_input = output\n",
    "\n",
    "\t# the fully encoded and reconstructed value of x is here:\n",
    "\treconstructed_x = next_layer_input\n",
    "\n",
    "\treturn {\n",
    "\t\t'encoded': encoded_x,\n",
    "\t\t'decoded': reconstructed_x,\n",
    "\t\t'cost' : tf.sqrt(tf.reduce_mean(tf.square(x-reconstructed_x)))\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "start_dim = dimension\n",
    "x = tf.placeholder(\"float\", [None, start_dim])\n",
    "\n",
    "autoencoder = create(x, [100, 60, 30, 15, 8, 3, 2])\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(autoencoder['cost'])\n",
    "\n",
    "for i in range(25000):\n",
    "\tsess.run(train_step, feed_dict={x: np.array(to_reduce)})\n",
    "    \n",
    "encoded = sess.run(autoencoder['encoded'], feed_dict={x: np.array(to_reduce)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reduced = {}\n",
    "\n",
    "loop = 0\n",
    "for point in mapped_exist:\n",
    "    reduced[point] = encoded[loop]\n",
    "    loop = loop + 1\n",
    "    \n",
    "# print reduced\n",
    "\n",
    "min_x = 0\n",
    "max_x = 0\n",
    "min_y = 0\n",
    "max_y = 0\n",
    "# min_z = 0\n",
    "# max_z = 0\n",
    "\n",
    "for point in reduced:\n",
    "    coordinate = reduced[point]\n",
    "    \n",
    "    if coordinate[0] < min_x:\n",
    "        min_x = coordinate[0]\n",
    "    elif coordinate[0] > max_x:\n",
    "        max_x = coordinate[0]\n",
    "        \n",
    "    if coordinate[1] < min_y:\n",
    "        min_y = coordinate[1]\n",
    "    elif coordinate[1] > max_y:\n",
    "        max_y = coordinate[1]\n",
    "        \n",
    "#     if coordinate[2] < min_z:\n",
    "#         min_z = coordinate[2]\n",
    "#     elif coordinate[2] > max_z:\n",
    "#         max_z = coordinate[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as pyplot\n",
    "import pylab\n",
    "\n",
    "fig = pylab.figure(figsize=(16, 20))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "spread = 0\n",
    "for x in mapped_flat:\n",
    "    for y in mapped_flat[x]:\n",
    "        coordinates = reduced[x*y]\n",
    "        ax.text(coordinates[0], coordinates[1], spread,\n",
    "                mapped_flat[x][y], size=6)\n",
    "        spread = spread + 0.01\n",
    "\n",
    "pyplot.xlim([min_x, 1.1*max_x])\n",
    "pyplot.ylim([min_y, max_y])\n",
    "ax.set_zlim(0, spread)\n",
    "\n",
    "ax.set_xticks([])                               \n",
    "ax.set_yticks([])                               \n",
    "ax.set_zticks([])\n",
    "pyplot.savefig('SOM2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
