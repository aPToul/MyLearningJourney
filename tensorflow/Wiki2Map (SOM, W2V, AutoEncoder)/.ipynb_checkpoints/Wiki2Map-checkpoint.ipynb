{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bleach\n",
    "import requests\n",
    "\n",
    "# Wikipedia scraper from\n",
    "# http://www.gyford.com/phil/writing/2015/03/25/wikipedia-parsing.php\n",
    "\n",
    "class WikipediaFetcher(object):\n",
    "\n",
    "    def fetch(self, page_name):\n",
    "        \"\"\"\n",
    "        Passed a Wikipedia page's URL fragment, like\n",
    "        'Edward_Montagu,_1st_Earl_of_Sandwich', this will fetch the page's\n",
    "        main contents, tidy the HTML, strip out any elements we don't want\n",
    "        and return the final HTML string.\n",
    "\n",
    "        Returns a dict with two elements:\n",
    "            'success' is either True or, if we couldn't fetch the page, False.\n",
    "            'content' is the HTML if success==True, or else an error message.\n",
    "        \"\"\"\n",
    "        result = self._get_html(page_name)\n",
    "        \n",
    "        if result['success']:\n",
    "            result['content'] = self._tidy_html(result['content'])\n",
    "            \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def _get_html(self, page_name):\n",
    "        \"\"\"\n",
    "        Passed the name of a Wikipedia page (eg, 'Samuel_Pepys'), it fetches\n",
    "        the HTML content (not the entire HTML page) and returns it.\n",
    "\n",
    "        Returns a dict with two elements:\n",
    "            'success' is either True or, if we couldn't fetch the page, False.\n",
    "            'content' is the HTML if success==True, or else an error message.\n",
    "        \"\"\"\n",
    "        error_message = ''\n",
    "\n",
    "        url = 'https:%s' % page_name\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params={'action':'render'}, timeout=5)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            error_message = \"Can't connect to domain.\"\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            error_message = \"Connection timed out.\"\n",
    "        except requests.exceptions.TooManyRedirects as e:\n",
    "            error_message = \"Too many redirects.\"\n",
    "\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # 4xx or 5xx errors:\n",
    "            error_message = \"HTTP Error: %s\" % response.status_code\n",
    "        except NameError:\n",
    "            if error_message == '':\n",
    "                error_message = \"Something unusual went wrong.\"\n",
    "\n",
    "        if error_message:\n",
    "            return {'success': False, 'content': error_message} \n",
    "        else:\n",
    "            return {'success': True, 'content': response.text}\n",
    "\n",
    "    def _tidy_html(self, html):\n",
    "        \"\"\"\n",
    "        Passed the raw Wikipedia HTML, this returns valid HTML, with all\n",
    "        disallowed elements stripped out.\n",
    "        \"\"\"\n",
    "        #html = self._bleach_html(html)\n",
    "        #html = self._strip_html(html)\n",
    "        return html\n",
    "\n",
    "    def _bleach_html(self, html):\n",
    "        \"\"\"\n",
    "        Ensures we have valid HTML; no unclosed or mis-nested tags.\n",
    "        Removes any tags and attributes we don't want to let through.\n",
    "        Doesn't remove the contents of any disallowed tags.\n",
    "\n",
    "        Pass it an HTML string, it'll return the bleached HTML string.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pretty much most elements, but no forms or audio/video.\n",
    "        allowed_tags = [\n",
    "            'a', 'abbr', 'acronym', 'address', 'area', 'article',\n",
    "            'b', 'blockquote', 'br',\n",
    "            'caption', 'cite', 'code', 'col', 'colgroup',\n",
    "            'dd', 'del', 'dfn', 'div', 'dl', 'dt',\n",
    "            'em',\n",
    "            'figcaption', 'figure', 'footer',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'header', 'hgroup', 'hr',\n",
    "            'i', 'img', 'ins',\n",
    "            'kbd',\n",
    "            'li',\n",
    "            'map',\n",
    "            'nav',\n",
    "            'ol',\n",
    "            'p', 'pre',\n",
    "            'q',\n",
    "            's', 'samp', 'section', 'small', 'span', 'strong', 'sub', 'sup',\n",
    "            'table', 'tbody', 'td', 'tfoot', 'th', 'thead', 'time', 'tr',\n",
    "            'ul',\n",
    "            'var',\n",
    "        ]\n",
    "\n",
    "        # These attributes will be removed from any of the allowed tags.\n",
    "        allowed_attributes = {\n",
    "            '*':        ['class', 'id'],\n",
    "            'a':        ['href', 'title'],\n",
    "            'abbr':     ['title'],\n",
    "            'acronym':  ['title'],\n",
    "            'img':      ['alt', 'src', 'srcset'],\n",
    "            # Ugh. Don't know why this page doesn't use .tright like others\n",
    "            # http://127.0.0.1:8000/encyclopedia/5040/\n",
    "            'table':    ['align'],\n",
    "            'td':       ['colspan', 'rowspan'],\n",
    "            'th':       ['colspan', 'rowspan', 'scope'],\n",
    "        }\n",
    "\n",
    "        return bleach.clean(html, tags=allowed_tags,\n",
    "                                    attributes=allowed_attributes, strip=True)\n",
    "\n",
    "    def _strip_html(self, html):\n",
    "        \"\"\"\n",
    "        Takes out any tags, and their contents, that we don't want at all.\n",
    "        And adds custom classes to existing tags (so we can apply CSS styles\n",
    "        without having to multiply our CSS).\n",
    "\n",
    "        Pass it an HTML string, it returns the stripped HTML string.\n",
    "        \"\"\"\n",
    "\n",
    "        # CSS selectors. Strip these and their contents.\n",
    "        selectors = [\n",
    "            'div.hatnote',\n",
    "            'div.navbar.mini', # Will also match div.mini.navbar\n",
    "            # Bottom of https://en.wikipedia.org/wiki/Charles_II_of_England :\n",
    "            'div.topicon',\n",
    "            'a.mw-headline-anchor',\n",
    "        ]\n",
    "\n",
    "        # Strip any element that has one of these classes.\n",
    "        classes = [\n",
    "            # \"This article may be expanded with text translated from...\"\n",
    "            # https://en.wikipedia.org/wiki/Afonso_VI_of_Portugal\n",
    "            'ambox-notice',\n",
    "            'magnify',\n",
    "            # eg audio on https://en.wikipedia.org/wiki/Bagpipes\n",
    "            'mediaContainer',\n",
    "            'navbox',\n",
    "            'noprint',\n",
    "        ]\n",
    "\n",
    "        # Any element has a class matching a key, it will have the classes\n",
    "        # in the value added.\n",
    "        add_classes = {\n",
    "            # Give these tables standard Bootstrap styles.\n",
    "            'infobox':   ['table', 'table-bordered'],\n",
    "            'ambox':     ['table', 'table-bordered'],\n",
    "            'wikitable': ['table', 'table-bordered'],\n",
    "        } \n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        for selector in selectors:\n",
    "            [tag.decompose() for tag in soup.select(selector)]\n",
    "\n",
    "        for clss in classes:\n",
    "            [tag.decompose() for tag in soup.find_all(attrs={'class':clss})]\n",
    "\n",
    "        for clss, new_classes in add_classes.iteritems():\n",
    "            for tag in soup.find_all(attrs={'class':clss}):\n",
    "                tag['class'] = tag.get('class', []) + new_classes\n",
    "\n",
    "        # Depending on the HTML parser BeautifulSoup used, soup may have\n",
    "        # surrounding <html><body></body></html> or just <body></body> tags.\n",
    "        if soup.body:\n",
    "            soup = soup.body\n",
    "        elif soup.html:\n",
    "            soup = soup.html.body\n",
    "\n",
    "        # Put the content back into a string.\n",
    "        html = ''.join(str(tag) for tag in soup.contents)\n",
    "\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//en.wikipedia.org/wiki/AODE', '//en.wikipedia.org/wiki/Artificial_neural_network', '//en.wikipedia.org/wiki/Backpropagation', '//en.wikipedia.org/wiki/Autoencoder', '//en.wikipedia.org/wiki/Hopfield_network', '//en.wikipedia.org/wiki/Boltzmann_machine', '//en.wikipedia.org/wiki/Restricted_Boltzmann_Machine', '//en.wikipedia.org/wiki/Spiking_neural_network', '//en.wikipedia.org/wiki/Bayesian_statistics', '//en.wikipedia.org/wiki/Bayesian_network', '//en.wikipedia.org/w/index.php?title=Bayesian_knowledge_base&action=edit&redlink=1', '//en.wikipedia.org/wiki/Case-based_reasoning', '//en.wikipedia.org/wiki/Gaussian_process_regression', '//en.wikipedia.org/wiki/Gene_expression_programming', '//en.wikipedia.org/wiki/Group_method_of_data_handling', '//en.wikipedia.org/wiki/Inductive_logic_programming', '//en.wikipedia.org/wiki/Instance-based_learning', '//en.wikipedia.org/wiki/Lazy_learning', '//en.wikipedia.org/wiki/Learning_Automata', '//en.wikipedia.org/wiki/Learning_Vector_Quantization', '//en.wikipedia.org/wiki/Logistic_Model_Tree', '//en.wikipedia.org/wiki/Minimum_message_length', '//en.wikipedia.org/wiki/Nearest_neighbor_(pattern_recognition)', '//en.wikipedia.org/wiki/Analogical_modeling', '//en.wikipedia.org/wiki/Probably_approximately_correct_learning', '//en.wikipedia.org/wiki/Ripple_down_rules', '//en.wikipedia.org/w/index.php?title=Symbolic_machine_learning&action=edit&redlink=1', '//en.wikipedia.org/wiki/Support_vector_machine', '//en.wikipedia.org/wiki/Random_forest', '//en.wikipedia.org/wiki/Ensembles_of_classifiers', '//en.wikipedia.org/wiki/Bootstrap_aggregating', '//en.wikipedia.org/wiki/Boosting_(meta-algorithm)', '//en.wikipedia.org/wiki/Ordinal_classification', '//en.wikipedia.org/wiki/Information_Fuzzy_Networks', '//en.wikipedia.org/wiki/Conditional_Random_Field', '//en.wikipedia.org/wiki/ANOVA', '//en.wikipedia.org/wiki/Linear_classifier', '//en.wikipedia.org/wiki/Fisher%27s_linear_discriminant', '//en.wikipedia.org/wiki/Logistic_regression', '//en.wikipedia.org/wiki/Multinomial_logistic_regression', '//en.wikipedia.org/wiki/Naive_Bayes_classifier', '//en.wikipedia.org/wiki/Perceptron', '//en.wikipedia.org/wiki/Support_vector_machine', '//en.wikipedia.org/wiki/Quadratic_classifier', '//en.wikipedia.org/wiki/Nearest_neighbor_(pattern_recognition)', '//en.wikipedia.org/wiki/Boosting_(machine_learning)', '//en.wikipedia.org/wiki/Decision_tree_learning', '//en.wikipedia.org/wiki/C4.5', '//en.wikipedia.org/wiki/Random_forest', '//en.wikipedia.org/wiki/ID3_algorithm', '//en.wikipedia.org/wiki/Classification_and_regression_tree', '//en.wikipedia.org/w/index.php?title=SLIQ&action=edit&redlink=1', '//en.wikipedia.org/w/index.php?title=SPRINT_(machine_learning)&action=edit&redlink=1', '//en.wikipedia.org/wiki/Bayesian_network', '//en.wikipedia.org/wiki/Naive_Bayes', '//en.wikipedia.org/wiki/Hidden_Markov_model', '//en.wikipedia.org/wiki/Unsupervised_learning', '//en.wikipedia.org/wiki/Expectation-maximization_algorithm', '//en.wikipedia.org/wiki/Vector_Quantization', '//en.wikipedia.org/wiki/Generative_topographic_map', '//en.wikipedia.org/wiki/Information_bottleneck_method', '//en.wikipedia.org/wiki/Artificial_neural_network', '//en.wikipedia.org/wiki/Self-organizing_map', '//en.wikipedia.org/wiki/Association_rule_learning', '//en.wikipedia.org/wiki/Apriori_algorithm', '//en.wikipedia.org/wiki/Eclat_algorithm', '//en.wikipedia.org/wiki/Association_rule_learning#FP-growth_algorithm', '//en.wikipedia.org/wiki/Hierarchical_clustering', '//en.wikipedia.org/wiki/Single-linkage_clustering', '//en.wikipedia.org/wiki/Conceptual_clustering', '//en.wikipedia.org/wiki/Cluster_analysis', '//en.wikipedia.org/wiki/K-means_algorithm', '//en.wikipedia.org/wiki/Fuzzy_clustering', '//en.wikipedia.org/wiki/DBSCAN', '//en.wikipedia.org/wiki/OPTICS_algorithm', '//en.wikipedia.org/wiki/Local_Outlier_Factor', '//en.wikipedia.org/wiki/Semi-supervised_learning', '//en.wikipedia.org/wiki/Semi-supervised_learning#Generative_models', '//en.wikipedia.org/wiki/Semi-supervised_learning#Low-density_separation', '//en.wikipedia.org/wiki/Semi-supervised_learning#Graph-based_methods', '//en.wikipedia.org/wiki/Co-training', '//en.wikipedia.org/wiki/Reinforcement_learning', '//en.wikipedia.org/wiki/Temporal_difference_learning', '//en.wikipedia.org/wiki/Q-learning', '//en.wikipedia.org/wiki/Learning_Automata', '//en.wikipedia.org/wiki/State-Action-Reward-State-Action', '//en.wikipedia.org/wiki/Deep_learning', '//en.wikipedia.org/wiki/Deep_belief_network', '//en.wikipedia.org/wiki/Boltzmann_machine', '//en.wikipedia.org/wiki/Convolutional_neural_network', '//en.wikipedia.org/wiki/Recurrent_neural_network', '//en.wikipedia.org/wiki/Hierarchical_temporal_memory', '//en.wikipedia.org/wiki/Data_Pre-processing', '//en.wikipedia.org/wiki/List_of_artificial_intelligence_projects', '//en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research']\n"
     ]
    }
   ],
   "source": [
    "myScraper = WikipediaFetcher()\n",
    "scraped_article = myScraper.fetch(page_name=\"//en.wikipedia.org/wiki/List_of_machine_learning_concepts\")\n",
    "\n",
    "links = []\n",
    "if scraped_article['success'] == True:\n",
    "    soup = BeautifulSoup(scraped_article['content'], 'html.parser')\n",
    "    \n",
    "    for a in soup.find_all('a', href=True):\n",
    "        links.append(a['href'])\n",
    "\n",
    "# Remove first 2 links which are not useful in this case\n",
    "links = [str(link) for link in links[14:]]\n",
    "print links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Bayesian_statistics -- was too short: \n",
      "\n",
      " Bayesian statistics, named for Thomas Bayes (1701-1761), is a theory in the field of statistics in which the evidence about the true state of the world is expressed in terms of 'degrees of belief' called Bayesian probabilities. Such an interpretation is only one of a number of interpretations of probability and there are other statistical techniques that are not based on 'degrees of belief'. One of the key ideas of Bayesian statistics is that \"probability is orderly opinion, and that inference from data is nothing other than the revision of such opinion in the light of relevant new information.\"[1]   The general set of statistical techniques can be divided into a number of activities, many of which have special Bayesian versions. Bayesian inference is an approach to statistical inference, that is distinct from frequentist inference. It is specifically based on the use of Bayesian probabilities to summarize evidence. The formulation of statistical models using Bayesian statistics has the unique feature of requiring the specification of prior distributions for any unknown parameters. These prior distributions are as integral to a Bayesian approach to statistical modelling as the expression of probability distributions. Prior distributions can be either hyperparameters or hyperprior distributions. The Bayesian design of experiments includes a concept called 'influence of prior beliefs'. This approach uses sequential analysis techniques to include the outcome of earlier experiments in the design of the next experiment. This is achieved by updating 'beliefs' through the use of prior and posterior distribution. This allows the design of experiments to make good use of resources of all types. An example of this is the multi-armed bandit problem. Statistical graphics includes methods for data exploration, for model validation, etc. The use of certain modern computational techniques for Bayesian inference, specifically the various types of Markov chain Monte Carlo techniques, have led to the need for checks, often made in graphical form, on the validity of such computations in expressing the required posterior distributions.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/w/index.php?title=Bayesian_knowledge_base&action=edit&redlink=1 -- failed to scrape\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Instance-based_learning -- was too short: \n",
      "\n",
      " In machine learning, instance-based learning (sometimes called memory-based learning[1]) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory. Instance-based learning is a kind of lazy learning. It is called instance-based because it constructs hypotheses directly from the training instances themselves.[2] This means that the hypothesis complexity can grow with the data:[2] in the worst case, a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is O(n). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data: instance-based learners may simply store a new instance or throw an old instance away. Examples of instance-based learning algorithm are the k-nearest neighbor algorithm, kernel machines and RBF networks.[3]:ch. 8 These store (a subset of) their training set; when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision. To battle the memory complexity of storing all training instances, as well as the risk of overfitting to noise in the training set, instance reduction algorithms have been proposed.[4] Gagliardi[5] applies this family of classifiers in medical field as second-opinion diagnostic tools and as tools for the knowledge extraction phase in the process of knowledge discovery in databases. One of these classifiers (called Prototype exemplar learning classifier (PEL-C) is able to extract a mixture of abstracted prototypical cases (that are syndromes) and selected atypical clinical cases.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Lazy_learning -- was too short: \n",
      "\n",
      " In machine learning, lazy learning is a learning method in which generalization beyond the training data is delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries. The main advantage gained in employing a lazy learning method, such as Case based reasoning, is that the target function will be approximated locally, such as in the k-nearest neighbor algorithm. Because the target function is approximated locally for each query to the system, lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain. The disadvantages with lazy learning include the large space requirement to store the entire training dataset. Particularly noisy training data increases the case base unnecessarily, because no abstraction is made during the training phase. Another disadvantage is that lazy learning methods are usually slower to evaluate, though this is coupled with a faster training phase. Lazy classifiers are most useful for large datasets with few attributes.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Learning_Vector_Quantization -- was too short: \n",
      "\n",
      " In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.   LVQ can be understood as a special case of an artificial neural network, more precisely, it applies a winner-take-all Hebbian learning-based approach. It is a precursor to self-organizing maps (SOM) and related to neural gas, and to the k-Nearest Neighbor algorithm (k-NN). LVQ was invented by Teuvo Kohonen.[1] An LVQ system is represented by prototypes  which are defined in the feature space of observed data. In winner-take-all training algorithms one determines, for each data point, the prototype which is closest to the input according to a given distance measure. The position of this so-called winner prototype is then adapted, i.e. the winner is moved closer if it correctly classifies the data point or moved away if it classifies the data point incorrectly. An advantage of LVQ is that it creates prototypes that are easy to interpret for experts in the respective application domain.[2] LVQ systems can be applied to multi-class classification problems in a natural way. It is used in a variety of practical applications. See http://liinwww.ira.uka.de/bibliography/Neural/SOM.LVQ.html for an extensive bibliography. A key issue in LVQ is the choice of an appropriate measure of distance or similarity for training and classification. Recently, techniques have been developed which adapt a parameterized distance measure in the course of training the system, see e.g. (Schneider, Biehl, and Hammer, 2009)[3] and references therein. LVQ can be a source of great help in classifying text documents.[4] Below follows an informal description.\n",
      "The algorithm consists of 3 basic steps. The algorithm's input is: The algorithm's flow is: Note:  and  are vectors in feature space.\n",
      "A more formal description can be found here: http://jsalatas.ictpro.gr/implementation-of-competitive-learning-networks-for-weka/\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Logistic_Model_Tree -- was too short: \n",
      "\n",
      " In computer science, a logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning.[1][2] Logistic model trees are based on the earlier idea of a model tree: a decision tree that has linear regression models at its leaves to provide a piecewise linear regression model (where ordinary decision trees with constants at their leaves would produce a piecewise constant model).[1] In the logistic variant, the LogitBoost algorithm is used to produce an LR model at every node in the tree; the node is then split using the C4.5 criterion. Each LogitBoost invocation is warm-started from its results in the parent node. Finally, the tree is pruned.[3] The basic LMT induction algorithm uses cross-validation to find a number of LogitBoost iterations that does not overfit the training data. A faster version has been proposed that uses the Akaike information criterion to control LogitBoost stopping.[3] \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/w/index.php?title=Symbolic_machine_learning&action=edit&redlink=1 -- failed to scrape\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Information_Fuzzy_Networks -- was too short: \n",
      "\n",
      " Info Fuzzy Networks (IFN) is a greedy machine learning algorithm for supervised learning. The data structure produced by the learning algorithm is also called Info Fuzzy Network. IFN construction is quite similar to decision trees' construction. However, IFN constructs a directed graph and not a tree. IFN also uses the conditional mutual information metric in order to choose features during the construction stage while decision trees usually use other metrics like entropy or gini.   Input: a list of input variables that can be used, a list of data records (training set) and a minimal statistical significance used to decide whether to split a node or not (default 0.1%).\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/C4.5 -- was too short: \n",
      "\n",
      " C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan.[1] C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. It became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008.[2]   C4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy. The training data is a set  of already classified samples. Each sample  consists of a p-dimensional vector , where the  represent attribute values or features of the sample, as well as the class in which  falls. At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurs on the smaller sublists. This algorithm has a few base cases. In pseudocode, the general algorithm for building decision trees is:[3] J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool. C4.5 made a number of improvements to ID3. Some of these are: Quinlan went on to create C5.0 and See5 (C5.0 for Unix/Linux, See5 for Windows) which he markets commercially. C5.0 offers a number of improvements on C4.5. Some of these are:[5][6] Source for a single-threaded Linux version of C5.0 is available under the GPL.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/w/index.php?title=SLIQ&action=edit&redlink=1 -- failed to scrape\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/w/index.php?title=SPRINT_(machine_learning)&action=edit&redlink=1 -- failed to scrape\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Deep_belief_network -- was too short: \n",
      "\n",
      " In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a type of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer.[1] When trained on a set of examples in an unsupervised way, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors on inputs.[1] After this learning step, a DBN can be further trained in a supervised way to perform classification.[2] DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs)[1] or autoencoders,[3] where each sub-network's hidden layer serves as the visible layer for the next. This also leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the \"lowest\" pair of layers (the lowest visible layer being a training set). The observation, due to Yee-Whye Teh, Geoffrey Hinton's student,[2] that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms.[4]:6   The training algorithm for DBNs proceeds as follows.[2] Let X be a matrix of inputs, regarded as a set of feature vectors.\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/Data_Pre-processing -- was too short: \n",
      "\n",
      " Data pre-processing is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis.[1] If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data pre-processing includes cleaning, normalization, transformation, feature extraction and selection, etc. The product of data pre-processing is the final training set. Kotsiantis et al. (2006) present a well-known algorithm for each step of data pre-processing.[2]\n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/List_of_artificial_intelligence_projects -- was too short: \n",
      "\n",
      " The following is a list of current and past, nonclassified notable artificial intelligence projects.  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "//en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research -- was too short: \n",
      "\n",
      " These datasets are used for machine learning research and have been cited in peer-reviewed academic journals and other publications. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets.[1] High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.[2][3][4][5] This list aggregates high-quality datasets that have been shown to be of value to the machine learning research community from multiple different data repositories to provide greater coverage of the topic than is otherwise available.   Datasets consisting primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification. In computer vision, facial images have been used extensively to develop facial recognition systems, face detection, and many other projects that use facial images. Images, text Images, text Images, text Datasets consisting primarily of text for tasks such as natural language processing, sentiment analysis, translation, and cluster analysis. citation analysis Datasets of sounds and sound features. (WAV) Datasets containing electric signal information requiring some sort of Signal processing for further analysis. Datasets from physical systems Datasets from biological systems. Datasets consisting of rows of observations and columns of attributes characterizing those observations. Typically used for regression analysis or classification but other types of algorithms can also be used. This section includes datasets that do not fit in the above categories.\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_set = {}\n",
    "\n",
    "# Scrape each article\n",
    "for article in links:\n",
    "    myScraper = WikipediaFetcher()\n",
    "    scraped_article = myScraper.fetch(page_name=article)\n",
    "    \n",
    "    # If succesfully scraped\n",
    "    if scraped_article['success'] == True:\n",
    "        \n",
    "        # Use beautiful soup to parse HTML\n",
    "        soup = BeautifulSoup(scraped_article['content'], 'html.parser')\n",
    "        \n",
    "        # FInd all p tags which seem to contain the body of the articles\n",
    "        get_tags = soup.find_all(['p'])        \n",
    "        \n",
    "        document_text = \"\"        \n",
    "        # For each tag found, strip extra characters and append to a document text string\n",
    "        for tag in get_tags:    \n",
    "            document_text = document_text + \" \" + (''.join(tag.findAll(text=True))).strip()                \n",
    "        \n",
    "        # If the document is of sufficient size\n",
    "        if len(document_text) > 250:                \n",
    "            document_set[article]  = document_text\n",
    "        else:\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print article + \" -- was too short: \"\n",
    "            print \"\"\n",
    "            print document_text\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print \"\"\n",
    "            \n",
    "    else:\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print article + \" -- failed to scrape\"\n",
    "            print \"----------------------------------------------------------------\"\n",
    "            print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from random import shuffle\n",
    "\n",
    "dataset = document_set.values()\n",
    "\n",
    "# Check for and remove duplicates \n",
    "dupes = [item for item, count in collections.Counter(dataset).items() if count > 1]\n",
    "\n",
    "# Shuffle order for model training\n",
    "if len(dupes) > 0:\n",
    "    print \"Duplicates found.\"\n",
    "    dataset = list(set(dataset))\n",
    "    shuffle(dataset)\n",
    "else:\n",
    "    shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Tokenizer For Clustering ###\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Porter stemmer is a pre-trained stemmer that finds the most likely stem for an english word\n",
    "from gensim.parsing import PorterStemmer\n",
    "global_stemmer = PorterStemmer()\n",
    "\n",
    "# Make a helper that will return the original form of the stem in future methods\n",
    "class StemmingHelper(object):\n",
    "    \"\"\"\n",
    "    Class to aid the stemming process - from word to stemmed form,\n",
    "    and vice versa.\n",
    "    The 'original' form of a stemmed word will be returned as the\n",
    "    form in which its been used the most number of times in the text.\n",
    "    \"\"\"\n",
    " \n",
    "    #This reverse lookup will remember the original forms of the stemmed words\n",
    "    word_lookup = {}\n",
    " \n",
    "    @classmethod\n",
    "    def stem(cls, word):\n",
    "        \"\"\"\n",
    "        Stems a word and updates the reverse lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        #Stem the word\n",
    "        stemmed = global_stemmer.stem(word)\n",
    " \n",
    "        #Update the word lookup\n",
    "        if stemmed not in cls.word_lookup:\n",
    "            cls.word_lookup[stemmed] = {}\n",
    "        cls.word_lookup[stemmed][word] = (\n",
    "            cls.word_lookup[stemmed].get(word, 0) + 1)\n",
    " \n",
    "        return stemmed\n",
    " \n",
    "    @classmethod\n",
    "    def original_form(cls, word):\n",
    "        \"\"\"\n",
    "        Returns original form of a word given the stemmed version,\n",
    "        as stored in the word lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        if word in cls.word_lookup:\n",
    "            return max(cls.word_lookup[word].keys(),\n",
    "                       key=lambda x: cls.word_lookup[word][x])\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "# Remove unicode, normalizing it to ascii\n",
    "import unicodedata\n",
    "\n",
    "# Custom tokenizer which produces sentence-wise tokens\n",
    "def sentence_tokenize(doc):\n",
    "        doc = unicodedata.normalize('NFKD', doc).encode('ascii','ignore')\n",
    "        doc = doc.lower()\n",
    "        doc = re.sub('[-—]', ' ', doc)\n",
    "        doc = re.sub(' +',' ', doc)\n",
    "                \n",
    "        sentences = nltk.sent_tokenize(doc) \n",
    "        \n",
    "        sentence_token_list = []\n",
    "        for sentence in sentences:\n",
    "            # Filter sentences with less than 5 words since they won't provide much context\n",
    "            if len(sentence) > 4:                \n",
    "                sentence_token_list.append([\n",
    "                        StemmingHelper.stem(word) \n",
    "                        for word in nltk.word_tokenize(sentence)\n",
    "                        # Filter words with no characters since they usually are numeric gibberish\n",
    "                        if len(re.sub('[^a-z]', '', word)) > 0\n",
    "                    ])       \n",
    "\n",
    "        return sentence_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build massive list of  sentences that are tokenized\n",
    "corpus = []\n",
    "for text in dataset:\n",
    "    corpus.extend(sentence_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimension = 60\n",
    "model = Word2Vec(corpus, size=dimension, window=5, min_count=5, sg=1, sample=0.0001, workers=4, iter=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1776\n"
     ]
    }
   ],
   "source": [
    "vocab = list(model.vocab.keys())\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar to: vector\n",
      "support\n",
      "dimensional\n",
      "dot\n",
      "map\n",
      "multidimensional\n",
      "------------------------------------------\n",
      "Similar to: clustering\n",
      "linkage\n",
      "dbscan\n",
      "distance\n",
      "optics\n",
      "dendrogram\n",
      "------------------------------------------\n",
      "Similar to: statistical\n",
      "journal\n",
      "packages\n",
      "science\n",
      "ordinal\n",
      "psychology\n",
      "------------------------------------------\n",
      "Similar to: model\n",
      "the\n",
      "network\n",
      "used\n",
      "general\n",
      "which\n",
      "------------------------------------------\n",
      "Similar to: deep\n",
      "neural\n",
      "learning\n",
      "architecture\n",
      "network\n",
      "feedforward\n",
      "------------------------------------------\n",
      "Similar to: neural\n",
      "network\n",
      "artificial\n",
      "recurrent\n",
      "feedforward\n",
      "deep\n",
      "------------------------------------------\n",
      "Similar to: network\n",
      "neural\n",
      "artificial\n",
      "layer\n",
      "convolutional\n",
      "feedforward\n",
      "------------------------------------------\n",
      "Similar to: training\n",
      "labeled\n",
      "data\n",
      "huge\n",
      "ctc\n",
      "set\n",
      "------------------------------------------\n",
      "Similar to: supervised\n",
      "semi\n",
      "learning\n",
      "unsupervised\n",
      "transductive\n",
      "pre\n",
      "------------------------------------------\n",
      "Similar to: unsupervised\n",
      "learning\n",
      "supervised\n",
      "semi\n",
      "paradigm\n",
      "ignored\n",
      "------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = \"vector cluster statistic model deep neural network train supervise unsupervise\"\n",
    "\n",
    "for stem in (StemmingHelper.stem(word) for word in test.split()):\n",
    "    print \"Similar to: \" + StemmingHelper.original_form(stem)\n",
    "    for similar in model.most_similar(stem)[0:5]:\n",
    "        print StemmingHelper.original_form(similar[0])\n",
    "    print \"------------------------------------------\"    \n",
    "print \"\"\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class SOM(object):\n",
    "    \"\"\"\n",
    "    2-D Self-Organizing Map\n",
    "    - Euclidean metric\n",
    "    - Gaussian Neighbourhood function\n",
    "    - Linearly decreasing learning rate\n",
    "    \"\"\"\n",
    " \n",
    "\n",
    "    #To check if the SOM has been trained\n",
    "    _trained = False\n",
    " \n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Initializes all components of the TensorFlow Graph.\n",
    "        - m X n are the dimensions of the SOM\n",
    "        - 'n_iterations' is an integer denoting # iterations for training\n",
    "        - 'dim' is the dimensionality of the training inputs\n",
    "        - 'alpha' is a number denoting the initial learning rate\n",
    "            >> default = 0.3\n",
    "        - 'sigma' is the the initial neighbourhood value\n",
    "            >> the radius of influence of the BMU while training\n",
    "            >> default = max(m, n) / 2.0\n",
    "        \"\"\"\n",
    " \n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    "\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "            \n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "            \n",
    "        # Normalizing constant for decay of alpha\n",
    "        time_constant = self._n_iterations/np.log(sigma)\n",
    "            \n",
    " \n",
    "        #Initialize Graph\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        #Populate graph\n",
    "        with self._graph.as_default():\n",
    "            ##Placeholders to be fed in during training \n",
    "            \n",
    "            #The training vector\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            #Iteration number\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            #Randomly initialized weights for neurons\n",
    "            #Stored as a matrix variable of shape [m*n, dim]\n",
    "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            #Grid locations of neurons\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "\n",
    "            #Compute the Best Matching Unit given a vector\n",
    "            #argmin of Euclidean distance b/w weights and given input\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.sub(self._weightage_vects, tf.pack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            #Extract locations\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]))),\n",
    "                                 [2])\n",
    " \n",
    "            #Adjust alpha and sigma values based on iteration\n",
    "            learning_rate_op = tf.exp(tf.neg(tf.div(self._iter_input,\n",
    "                                                  time_constant))) \n",
    "        \n",
    "            _alpha_op = tf.mul(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.mul(sigma, learning_rate_op)\n",
    " \n",
    "            #Generate learning rate for all neurons\n",
    "            #Based on iteration number and location wrt BMU\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.sub(\n",
    "                self._location_vects, tf.pack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            \n",
    "            neighbourhood_func = tf.exp(tf.neg(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            \n",
    "            learning_rate_op = tf.mul(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            #Update weight vectors of all neurons\n",
    "            learning_rate_multiplier = tf.pack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.mul(\n",
    "                learning_rate_multiplier,\n",
    "                tf.sub(tf.pack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects))                                         \n",
    "            \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            \n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            #Initialize session and variables\n",
    "            self._sess = tf.Session() \n",
    "            init_op = tf.initialize_all_variables()\n",
    "            self._sess.run(init_op)\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        \"\"\"\n",
    "        Yields one by one the 2-D locations of the individual neurons\n",
    "        in the SOM.\n",
    "        \"\"\"\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM\n",
    "        \"\"\"\n",
    " \n",
    "        #Training iterations\n",
    "        for iter_no in range(self._n_iterations):\n",
    "            #Train with each vector one by one\n",
    "            for input_vect in input_vects:\n",
    "                self._sess.run(self._training_op,\n",
    "                               feed_dict={self._vect_input: input_vect,\n",
    "                                          self._iter_input: iter_no})\n",
    " \n",
    "        #Store a centroid grid for easy retrieval later on\n",
    "        centroid_grid = [[] for i in range(self._m)]\n",
    "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "        self._locations = list(self._sess.run(self._location_vects))\n",
    "        for i, loc in enumerate(self._locations):\n",
    "            centroid_grid[loc[0]].append(self._weightages[i])\n",
    "        self._centroid_grid = centroid_grid\n",
    " \n",
    "        self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        \"\"\"\n",
    "        Returns a list of 'm' lists, with each inner list containing\n",
    "        the 'n' corresponding centroid locations as 1-D NumPy arrays.\n",
    "        \"\"\"\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        \"\"\"\n",
    "        Maps each input vector to neuron in the SOM grid.\n",
    "               \n",
    "        Returns a list of 1-D NumPy arrays containing (x, y) for each input.\n",
    "        \"\"\"\n",
    " \n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For plotting the images\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "myStop = stopwords.words('english')\n",
    "\n",
    "filtered_vocab = list(set(vocab)-set([global_stemmer.stem(x) for x in myStop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for sentence in corpus:\n",
    "    words.extend([x for x in sentence if x in filtered_vocab])\n",
    "\n",
    "from collections import Counter\n",
    "word_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used\n",
      "model\n",
      "learning\n",
      "data\n",
      "network\n",
      "algorithm\n",
      "set\n",
      "function\n",
      "clustering\n",
      "variables\n",
      "training\n",
      "values\n",
      "distribution\n",
      "neural\n",
      "example\n",
      "method\n",
      "general\n",
      "probability\n",
      "also\n",
      "input\n",
      "layer\n",
      "means\n",
      "problem\n",
      "two\n",
      "vector\n",
      "state\n",
      "based\n",
      "given\n",
      "weights\n",
      "different\n",
      "points\n",
      "number\n",
      "observed\n",
      "features\n",
      "linear\n",
      "case\n",
      "regression\n",
      "class\n",
      "computer\n",
      "parameters\n",
      "time\n",
      "classifier\n",
      "prediction\n",
      "deep\n",
      "following\n",
      "machine\n",
      "process\n",
      "tree\n",
      "results\n",
      "called\n",
      "error\n",
      "classification\n",
      "random\n",
      "rule\n",
      "units\n",
      "measure\n",
      "output\n",
      "nodes\n",
      "may\n",
      "sample\n",
      "neurons\n",
      "first\n",
      "estimate\n",
      "analysis\n",
      "distance\n",
      "represent\n",
      "optimal\n",
      "dependent\n",
      "hidden\n",
      "space\n",
      "system\n",
      "many\n",
      "statistical\n",
      "possible\n",
      "logistic\n",
      "however\n",
      "form\n",
      "term\n",
      "step\n",
      "associated\n",
      "large\n",
      "recognition\n",
      "way\n",
      "objects\n",
      "approach\n",
      "level\n",
      "connected\n",
      "make\n",
      "type\n",
      "often\n",
      "structure\n",
      "similar\n",
      "needed\n",
      "performance\n",
      "thus\n",
      "known\n",
      "expression\n",
      "applications\n",
      "conditional\n",
      "defined\n",
      "decision\n",
      "information\n",
      "sequence\n",
      "find\n",
      "include\n",
      "single\n",
      "combination\n",
      "labeled\n",
      "new\n",
      "order\n",
      "test\n",
      "activation\n",
      "since\n",
      "applied\n",
      "complex\n",
      "independent\n",
      "approximation\n",
      "map\n",
      "usually\n",
      "pattern\n",
      "tasks\n",
      "allows\n",
      "determine\n",
      "action\n",
      "like\n",
      "non\n",
      "programming\n",
      "gene\n",
      "likelihood\n",
      "requires\n",
      "representation\n",
      "item\n",
      "group\n",
      "effect\n",
      "separate\n",
      "local\n",
      "another\n",
      "related\n",
      "density\n",
      "image\n",
      "would\n",
      "fit\n",
      "selected\n",
      "particular\n",
      "work\n",
      "gradient\n",
      "squared\n",
      "considered\n",
      "variance\n",
      "described\n",
      "size\n",
      "expected\n",
      "factor\n",
      "common\n",
      "maximum\n",
      "initial\n",
      "speech\n",
      "artificial\n",
      "markov\n",
      "implemented\n",
      "assume\n",
      "techniques\n",
      "developed\n",
      "scale\n",
      "normal\n",
      "bayesian\n",
      "latent\n",
      "memory\n",
      "outcome\n",
      "multiple\n",
      "theory\n",
      "additional\n",
      "original\n",
      "standard\n",
      "take\n",
      "dimensional\n",
      "context\n",
      "matrix\n",
      "several\n",
      "consists\n",
      "assumptions\n",
      "research\n",
      "well\n",
      "perceptron\n",
      "converge\n",
      "contains\n",
      "iterative\n",
      "supervised\n",
      "concept\n",
      "create\n",
      "architecture\n",
      "ratio\n",
      "policy\n",
      "minimize\n",
      "produce\n",
      "convolutional\n",
      "see\n",
      "support\n",
      "typically\n",
      "solution\n"
     ]
    }
   ],
   "source": [
    "short_list = word_counts.most_common(200)\n",
    "for wordCount in short_list:\n",
    "    print StemmingHelper.original_form(wordCount[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "short_list = [StemmingHelper.stem(x) for x in 'algorithm activation approximation architecture artificial assumptions bayesian class classification classifier clustering conditional connected converge convolutional data decision deep density dependent dimensional distance distribution error estimate expected features fit function gene gradient hidden independent information iterative latent layer learning likelihood linear local logistic machine map markov matrix maximum means measure memory minimize model network neural neurons nodes normal optimal output parameters pattern perceptron prediction probability random recognition regression represent representation rule space squared standard state statistical structure supervised support test training tree variables variance vector weights'\n",
    "              .split()]\n",
    "\n",
    "short_list = list(set(short_list))\n",
    "print len(short_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_list = []\n",
    "\n",
    "for x in short_list:\n",
    "    vector_list.append(model[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = np.vstack(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training inputs\n",
    "\n",
    "grid_size = 6\n",
    "\n",
    "som = SOM(m=grid_size, n=int(grid_size), dim=dimension, n_iterations=5000)\n",
    "som.train(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Map to their closest neurons\n",
    "mapped = som.map_vects(vector_list)\n",
    "\n",
    "mapped_flat = {}\n",
    "for i, m in enumerate(mapped):\n",
    "    if not (m[0] in mapped_flat):\n",
    "        mapped_flat[m[0]] = {}\n",
    "    if not (m[1] in mapped_flat[m[0]]):\n",
    "        mapped_flat[m[0]][m[1]] = StemmingHelper.original_form(short_list[i])\n",
    "    else:\n",
    "        mapped_flat[m[0]][m[1]] = mapped_flat[m[0]][m[1]] + \"; \" + StemmingHelper.original_form(short_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get output grid\n",
    "grid = som.get_centroids()\n",
    "\n",
    "centroids = []\n",
    "for m_list in grid:\n",
    "    for centroid in m_list:\n",
    "        centroids.append(centroid)\n",
    "\n",
    "indexes = []\n",
    "for x in mapped_flat:\n",
    "    for y in mapped_flat[x]:\n",
    "        indexes.append(y*x)\n",
    "indexes = list(set(indexes))\n",
    "\n",
    "mapped_exist = {}\n",
    "for index in range(0, len(centroids)):\n",
    "    if index in indexes:\n",
    "        if not index in mapped_exist:\n",
    "            mapped_exist[index] = centroids[index]\n",
    "\n",
    "to_reduce = []\n",
    "for x in mapped_exist:\n",
    "    to_reduce.append(mapped_exist[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Deep Auto-Encoder implementation\n",
    "\t\n",
    "\tAn auto-encoder works as follows:\n",
    "\n",
    "\tData of dimension k is reduced to a lower dimension j using a matrix multiplication:\n",
    "\tsoftmax(W*x + b)  = x'\n",
    "\t\n",
    "\twhere W is matrix from R^k --> R^j\n",
    "\n",
    "\tA reconstruction matrix W' maps back from R^j --> R^k\n",
    "\n",
    "\tOur reconstruction function is softmax'(W' * x' + b') \n",
    "\n",
    "\tNow the point of the auto-encoder is to create a reduction matrix (values for W, b) \n",
    "\tthat is \"good\" at reconstructing  the original data. \n",
    "\n",
    "\tThus we want to minimize  ||softmax'(W' * (softmax(W *x+ b)) + b')  - x||\n",
    "\n",
    "\tA deep auto-encoder is nothing more than stacking successive layers of these reductions.\n",
    "\"\"\"\n",
    "import math\n",
    "import random\n",
    "\n",
    "def create(x, layer_sizes):\n",
    "\n",
    "\t# Build the encoding layers\n",
    "\tnext_layer_input = x\n",
    "\n",
    "\tencoding_matrices = []\n",
    "\tfor dim in layer_sizes:\n",
    "\t\tinput_dim = int(next_layer_input.get_shape()[1])\n",
    "\n",
    "\t\t# Initialize W using random values in interval [-1/sqrt(n) , 1/sqrt(n)]\n",
    "\t\tW = tf.Variable(tf.random_uniform([input_dim, dim], -1.0 / math.sqrt(input_dim), 1.0 / math.sqrt(input_dim)))\n",
    "\n",
    "\t\t# Initialize b to zero\n",
    "\t\tb = tf.Variable(tf.zeros([dim]))\n",
    "\n",
    "\t\t# We are going to use tied-weights so store the W matrix for later reference.\n",
    "\t\tencoding_matrices.append(W)\n",
    "\n",
    "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
    "\n",
    "\t\t# the input into the next layer is the output of this layer\n",
    "\t\tnext_layer_input = output\n",
    "\n",
    "\t# The fully encoded x value is now stored in the next_layer_input\n",
    "\tencoded_x = next_layer_input\n",
    "\n",
    "\t# build the reconstruction layers by reversing the reductions\n",
    "\tlayer_sizes.reverse()\n",
    "\tencoding_matrices.reverse()\n",
    "\n",
    "\n",
    "\tfor i, dim in enumerate(layer_sizes[1:] + [ int(x.get_shape()[1])]) :\n",
    "\t\t# we are using tied weights, so just lookup the encoding matrix for this step and transpose it\n",
    "\t\tW = tf.transpose(encoding_matrices[i])\n",
    "\t\tb = tf.Variable(tf.zeros([dim]))\n",
    "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
    "\t\tnext_layer_input = output\n",
    "\n",
    "\t# the fully encoded and reconstructed value of x is here:\n",
    "\treconstructed_x = next_layer_input\n",
    "\n",
    "\treturn {\n",
    "\t\t'encoded': encoded_x,\n",
    "\t\t'decoded': reconstructed_x,\n",
    "\t\t'cost' : tf.sqrt(tf.reduce_mean(tf.square(x-reconstructed_x)))\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "start_dim = 100\n",
    "x = tf.placeholder(\"float\", [None, start_dim])\n",
    "\n",
    "autoencoder = create(x, [50, 20, 10, 3, 2])\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(autoencoder['cost'])\n",
    "\n",
    "for i in range(5000):\n",
    "\tsess.run(train_step, feed_dict={x: np.array(to_reduce)})\n",
    "    \n",
    "encoded = sess.run(autoencoder['encoded'], feed_dict={x: np.array(to_reduce)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([ 0.00575317,  0.01391055], dtype=float32), 1: array([ 0.0032957 ,  0.00072033], dtype=float32), 2: array([ 0.00243847, -0.01659476], dtype=float32), 3: array([ 0.00247958, -0.02902633], dtype=float32), 4: array([ 0.00069504, -0.04273154], dtype=float32), 5: array([-0.00130949, -0.05165395], dtype=float32), 6: array([ 0.00577564,  0.00268402], dtype=float32), 8: array([ 0.00297177, -0.01729305], dtype=float32), 9: array([ 0.00313037, -0.028641  ], dtype=float32), 10: array([ 0.00082795, -0.0348777 ], dtype=float32), 12: array([ 0.00730333, -0.00059695], dtype=float32), 15: array([ 0.00081043, -0.01885062], dtype=float32), 16: array([ 0.000231  , -0.02117179], dtype=float32), 20: array([ 0.00089033, -0.0136381 ], dtype=float32), 25: array([ 0.00299081, -0.01782706], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "reduced = {}\n",
    "\n",
    "loop = 0\n",
    "for point in mapped_exist:\n",
    "    reduced[point] = encoded[loop]\n",
    "    loop = loop + 1\n",
    "    \n",
    "print reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_x = 0\n",
    "max_x = 0\n",
    "min_y = 0\n",
    "max_y = 0\n",
    "# min_z = 0\n",
    "# max_z = 0\n",
    "\n",
    "for point in reduced:\n",
    "    coordinate = reduced[point]\n",
    "    \n",
    "    if coordinate[0] < min_x:\n",
    "        min_x = coordinate[0]\n",
    "    elif coordinate[0] > max_x:\n",
    "        max_x = coordinate[0]\n",
    "        \n",
    "    if coordinate[1] < min_y:\n",
    "        min_y = coordinate[1]\n",
    "    elif coordinate[1] > max_y:\n",
    "        max_y = coordinate[1]\n",
    "        \n",
    "#     if coordinate[2] < min_z:\n",
    "#         min_z = coordinate[2]\n",
    "#     elif coordinate[2] > max_z:\n",
    "#         max_z = coordinate[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "# plt.title('Map of Concepts')\n",
    "# for x in mapped_flat:\n",
    "#     for y in mapped_flat[x]:\n",
    "#         coordinates = reduced[x*y]\n",
    "        \n",
    "#         plt.text(coordinates[0], coordinates[1], mapped_flat[x][y], ha='center', va='center')\n",
    "    \n",
    "# c = 1.2\n",
    "# plt.xlim(min_x*c, max_x*c)\n",
    "# plt.ylim(min_y*c, max_y*c)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = pylab.figure(figsize=(20.0, 10.0))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "count = 1\n",
    "for x in mapped_flat:\n",
    "    for y in mapped_flat[x]:\n",
    "        # temp\n",
    "        if not mapped_flat[x][y] == 'network':\n",
    "            coordinates = reduced[x*y]\n",
    "\n",
    "            ax.text(coordinates[0], coordinates[1], count,\n",
    "                    mapped_flat[x][y], size=8)\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "pyplot.xlim([min_x, 1.4*max_x])\n",
    "pyplot.ylim([min_y, max_y])\n",
    "ax.set_zlim(0, count)\n",
    "\n",
    "ax.set_xticks([])                               \n",
    "ax.set_yticks([])                               \n",
    "ax.set_zticks([])\n",
    "\n",
    "pyplot.savefig('output.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
